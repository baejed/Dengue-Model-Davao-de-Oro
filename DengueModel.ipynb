{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Preparing Pandas dataframe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dengue Cases\n",
       "0    10478\n",
       "1      943\n",
       "2      453\n",
       "3      110\n",
       "4       57\n",
       "8       29\n",
       "5       24\n",
       "6       23\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import helper_functions as helper\n",
    "\n",
    "dengue_df = pd.read_csv(\"ddo_dengue_data.csv\")\n",
    "\n",
    "# dengue_df = dengue_df.drop(labels='Baranggay', axis=1)\n",
    "dengue_df = dengue_df.drop(labels='Month', axis=1)\n",
    "dengue_df = dengue_df.drop(labels='Year', axis=1)\n",
    "dengue_df = dengue_df.drop(labels='Baranggay', axis=1)\n",
    "\n",
    "# baranggay_transform, baranggay_dict = helper.transform_classifications(dengue_df['Baranggay'])\n",
    "soil_type_transform, soil_type_dict = helper.transform_classifications(dengue_df['Soil Type'])\n",
    "elevation_transform, elevation_dict = helper.transform_elevation(dengue_df['Elevation'])\n",
    "\n",
    "dengue_df['Soil Type'] = soil_type_transform\n",
    "dengue_df['Elevation'] = elevation_transform\n",
    "# dengue_df['Baranggay'] = baranggay_transform\n",
    "dengue_df['Dengue Cases'].value_counts()\n",
    "\n",
    "# for i in range(len(dengue_df)):\n",
    "#     if dengue_df['Dengue Cases'][i] == 0 :\n",
    "#         dengue_df.drop(i, inplace=True)\n",
    "\n",
    "dengue_df['Dengue Cases'].value_counts()\n",
    "# dengue_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Converting pandas dataframe to torch tensor*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "X = dengue_df.drop(labels='Dengue Cases', axis=1)\n",
    "y = dengue_df['Dengue Cases']\n",
    "\n",
    "X = torch.tensor(X.values).to(dtype=torch.float32)\n",
    "y = torch.tensor(y.values).to(dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Split the dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Device agnostic code*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else device\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Creating the Neural Network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DengueCasesPredictorModel(nn.Module):\n",
    "    def __init__(self, input_channels = 8, hidden_units = 10, output_shape = 1):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=input_channels, out_features=hidden_units)\n",
    "        self.layer_2 = nn.Linear(in_features=hidden_units, out_features=hidden_units)\n",
    "        self.layer_3 = nn.Linear(in_features=hidden_units, out_features=hidden_units)\n",
    "        self.layer_4 = nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_2(x)\n",
    "        # x = self.relu(x)\n",
    "        x = self.layer_3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2, # 0.2 = 20% of data will be test & 80% will be train\n",
    "                                                    random_state=RANDOM_SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DengueCasesPredictorModel(\n",
       "  (layer_1): Linear(in_features=8, out_features=10, bias=True)\n",
       "  (layer_2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (layer_3): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (layer_4): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# model_path = Path('models/dengue_model.pth')\n",
    "\n",
    "model = DengueCasesPredictorModel()\n",
    "# model.load_state_dict(torch.load(model_path))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.00000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: nn.Module, loss_fn: nn.Module, optimizer: torch.optim.Optimizer, X, y):\n",
    "    model.train()\n",
    "\n",
    "    y_preds = model(X)\n",
    "    loss = loss_fn(y_preds, y)\n",
    "\n",
    "    # print(loss)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Train loss: {loss}\")\n",
    "\n",
    "    return y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: nn.Module, loss_fn: nn.Module, X, y):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        test_pred = model(X)\n",
    "        test_loss = loss_fn(test_pred, y)\n",
    "\n",
    "    print(f\"Test loss: {test_loss}\")\n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1311, 8])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([9693])) that is different to the input size (torch.Size([9693, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.24176910519599915\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 1\n",
      "Train loss: 0.24176910519599915\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([2424])) that is different to the input size (torch.Size([2424, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.24176910519599915\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 3\n",
      "Train loss: 0.24176910519599915\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 4\n",
      "Train loss: 0.24176910519599915\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 5\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 6\n",
      "Train loss: 0.24176910519599915\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 7\n",
      "Train loss: 0.24176910519599915\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 8\n",
      "Train loss: 0.24176910519599915\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 9\n",
      "Train loss: 0.24176910519599915\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 10\n",
      "Train loss: 0.24176910519599915\n",
      "Test loss: 0.24749873578548431\n",
      "----------------------\n",
      "Epoch: 11\n",
      "Train loss: 0.24176910519599915\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 12\n",
      "Train loss: 0.24176910519599915\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 13\n",
      "Train loss: 0.24176910519599915\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 14\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 15\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 16\n",
      "Train loss: 0.24176910519599915\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 17\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 18\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 19\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 20\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 21\n",
      "Train loss: 0.24176910519599915\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 22\n",
      "Train loss: 0.24176910519599915\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 23\n",
      "Train loss: 0.24176910519599915\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 24\n",
      "Train loss: 0.24176910519599915\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 25\n",
      "Train loss: 0.24176910519599915\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 26\n",
      "Train loss: 0.24176910519599915\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 27\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 28\n",
      "Train loss: 0.24176910519599915\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 29\n",
      "Train loss: 0.24176910519599915\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 30\n",
      "Train loss: 0.24176910519599915\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 31\n",
      "Train loss: 0.24176910519599915\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 32\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 33\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 34\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 35\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 36\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 37\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 38\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 39\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 40\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 41\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 42\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 43\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 44\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 45\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 46\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 47\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 48\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 49\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 50\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 51\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 52\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 53\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 54\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 55\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 56\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 57\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 58\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 59\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 60\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 61\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 62\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 63\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 64\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 65\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 66\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 67\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 68\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 69\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 70\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 71\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 72\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 73\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 74\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 75\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 76\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 77\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 78\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 79\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 80\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 81\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 82\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749872088432312\n",
      "----------------------\n",
      "Epoch: 83\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 84\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 85\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 86\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 87\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 88\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 89\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 90\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 91\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 92\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 93\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 94\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 95\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 96\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 97\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 98\n",
      "Train loss: 0.24176906049251556\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 99\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 100\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 101\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 102\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 103\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 104\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 105\n",
      "Train loss: 0.24176906049251556\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 106\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 107\n",
      "Train loss: 0.24176906049251556\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 108\n",
      "Train loss: 0.24176906049251556\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 109\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 110\n",
      "Train loss: 0.24176906049251556\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 111\n",
      "Train loss: 0.24176906049251556\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 112\n",
      "Train loss: 0.24176906049251556\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 113\n",
      "Train loss: 0.24176906049251556\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 114\n",
      "Train loss: 0.24176906049251556\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 115\n",
      "Train loss: 0.24176906049251556\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 116\n",
      "Train loss: 0.24176906049251556\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 117\n",
      "Train loss: 0.24176906049251556\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 118\n",
      "Train loss: 0.24176906049251556\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 119\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 120\n",
      "Train loss: 0.24176906049251556\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 121\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 122\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 123\n",
      "Train loss: 0.24176907539367676\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 124\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 125\n",
      "Train loss: 0.24176906049251556\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 126\n",
      "Train loss: 0.24176906049251556\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 127\n",
      "Train loss: 0.24176906049251556\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 128\n",
      "Train loss: 0.24176906049251556\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 129\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 130\n",
      "Train loss: 0.24176906049251556\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 131\n",
      "Train loss: 0.24176906049251556\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 132\n",
      "Train loss: 0.24176906049251556\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 133\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 134\n",
      "Train loss: 0.24176906049251556\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 135\n",
      "Train loss: 0.24176906049251556\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 136\n",
      "Train loss: 0.24176906049251556\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 137\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 138\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 139\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 140\n",
      "Train loss: 0.24176906049251556\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 141\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 142\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 143\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 144\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 145\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 146\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 147\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 148\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 149\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 150\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 151\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 152\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 153\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 154\n",
      "Train loss: 0.24176906049251556\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 155\n",
      "Train loss: 0.24176906049251556\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 156\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 157\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 158\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 159\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 160\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 161\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 162\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 163\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 164\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 165\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 166\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 167\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 168\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 169\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 170\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 171\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 172\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 173\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 174\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 175\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 176\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749869108200073\n",
      "----------------------\n",
      "Epoch: 177\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 178\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 179\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 180\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 181\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 182\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 183\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 184\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 185\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 186\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 187\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 188\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 189\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 190\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 191\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 192\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 193\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 194\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 195\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 196\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 197\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 198\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 199\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 200\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 201\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 202\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 203\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 204\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 205\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 206\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 207\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 208\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 209\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 210\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 211\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 212\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 213\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 214\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 215\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 216\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 217\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 218\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 219\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 220\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 221\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 222\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 223\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 224\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 225\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 226\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749867618083954\n",
      "----------------------\n",
      "Epoch: 227\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 228\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 229\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 230\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 231\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 232\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 233\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 234\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 235\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 236\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 237\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 238\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 239\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 240\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 241\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 242\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 243\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 244\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 245\n",
      "Train loss: 0.24176903069019318\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 246\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 247\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 248\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 249\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 250\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 251\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 252\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 253\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 254\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 255\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 256\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 257\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 258\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 259\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 260\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 261\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 262\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 263\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 264\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 265\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 266\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 267\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 268\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 269\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 270\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 271\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 272\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 273\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 274\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 275\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 276\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 277\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 278\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 279\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 280\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 281\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 282\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 283\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 284\n",
      "Train loss: 0.24176901578903198\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 285\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 286\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 287\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 288\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 289\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 290\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 291\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 292\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 293\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 294\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 295\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 296\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 297\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 298\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 299\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 300\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 301\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 302\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 303\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749866127967834\n",
      "----------------------\n",
      "Epoch: 304\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 305\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 306\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 307\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 308\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 309\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 310\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 311\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 312\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 313\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 314\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 315\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 316\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 317\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 318\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 319\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 320\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 321\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 322\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 323\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 324\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 325\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 326\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 327\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 328\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 329\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 330\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 331\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 332\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 333\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 334\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 335\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 336\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 337\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 338\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 339\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 340\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 341\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 342\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 343\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 344\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 345\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 346\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 347\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 348\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 349\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 350\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 351\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 352\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 353\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 354\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 355\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 356\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 357\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749863147735596\n",
      "----------------------\n",
      "Epoch: 358\n",
      "Train loss: 0.2417690008878708\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 359\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 360\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 361\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 362\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 363\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 364\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 365\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 366\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 367\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 368\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 369\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 370\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 371\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 372\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 373\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 374\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 375\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 376\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 377\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 378\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 379\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 380\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 381\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 382\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 383\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 384\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 385\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 386\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 387\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 388\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 389\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 390\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 391\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 392\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 393\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 394\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 395\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 396\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 397\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 398\n",
      "Train loss: 0.2417689710855484\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 399\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 400\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 401\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 402\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 403\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 404\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 405\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 406\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 407\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 408\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 409\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 410\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 411\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 412\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 413\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 414\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 415\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 416\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 417\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 418\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 419\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 420\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 421\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 422\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749861657619476\n",
      "----------------------\n",
      "Epoch: 423\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 424\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 425\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 426\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 427\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 428\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 429\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 430\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 431\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 432\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 433\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 434\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 435\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 436\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 437\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 438\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 439\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 440\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 441\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 442\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 443\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 444\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 445\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 446\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 447\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 448\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 449\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 450\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 451\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 452\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 453\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 454\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 455\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 456\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 457\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 458\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 459\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 460\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 461\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 462\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 463\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 464\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 465\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 466\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 467\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 468\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 469\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 470\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 471\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 472\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 473\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 474\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 475\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 476\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 477\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 478\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 479\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 480\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 481\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 482\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 483\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 484\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 485\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749858677387238\n",
      "----------------------\n",
      "Epoch: 486\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 487\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 488\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 489\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 490\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 491\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 492\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 493\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 494\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 495\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 496\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 497\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 498\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 499\n",
      "Train loss: 0.2417689561843872\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 500\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 501\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 502\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 503\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 504\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 505\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 506\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 507\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 508\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 509\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 510\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 511\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 512\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 513\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 514\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 515\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 516\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 517\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 518\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 519\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 520\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 521\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 522\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 523\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 524\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 525\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 526\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 527\n",
      "Train loss: 0.24176892638206482\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 528\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 529\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 530\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 531\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 532\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 533\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 534\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 535\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 536\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 537\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 538\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 539\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 540\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 541\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 542\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 543\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 544\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 545\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 546\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 547\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 548\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 549\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 550\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 551\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 552\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 553\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 554\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 555\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 556\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 557\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 558\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 559\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 560\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 561\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 562\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 563\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 564\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 565\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 566\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 567\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 568\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 569\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 570\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 571\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 572\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 573\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 574\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 575\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 576\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.24749857187271118\n",
      "----------------------\n",
      "Epoch: 577\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 578\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 579\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 580\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 581\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 582\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 583\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 584\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 585\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 586\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 587\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 588\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 589\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 590\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 591\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 592\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 593\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 594\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 595\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 596\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 597\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 598\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 599\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 600\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 601\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 602\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 603\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 604\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 605\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 606\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 607\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 608\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 609\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 610\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 611\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 612\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 613\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 614\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 615\n",
      "Train loss: 0.24176891148090363\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 616\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 617\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 618\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985420703888\n",
      "----------------------\n",
      "Epoch: 619\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 620\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 621\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 622\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 623\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 624\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 625\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 626\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 627\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 628\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 629\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 630\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 631\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 632\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 633\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 634\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 635\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 636\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 637\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 638\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 639\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 640\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 641\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 642\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 643\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 644\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 645\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 646\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 647\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 648\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 649\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 650\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 651\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 652\n",
      "Train loss: 0.24176888167858124\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 653\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 654\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 655\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 656\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 657\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 658\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 659\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 660\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985271692276\n",
      "----------------------\n",
      "Epoch: 661\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 662\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 663\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 664\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 665\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 666\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 667\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 668\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 669\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 670\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 671\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 672\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 673\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 674\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 675\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 676\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 677\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 678\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 679\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 680\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 681\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 682\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 683\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 684\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 685\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 686\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 687\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 688\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 689\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 690\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 691\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 692\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 693\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 694\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 695\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 696\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 697\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 698\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 699\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 700\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 701\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 702\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 703\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 704\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 705\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 706\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 707\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 708\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 709\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 710\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 711\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 712\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 713\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 714\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 715\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 716\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 717\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 718\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 719\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 720\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 721\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 722\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 723\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 724\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 725\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 726\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 727\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 728\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 729\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 730\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 731\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 732\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 733\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 734\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 735\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 736\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 737\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 738\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 739\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 740\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 741\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 742\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 743\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 744\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 745\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 746\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 747\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 748\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 749\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 750\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 751\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 752\n",
      "Train loss: 0.24176886677742004\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 753\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 754\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 755\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 756\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 757\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 758\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 759\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 760\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 761\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 762\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 763\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 764\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 765\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 766\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 767\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 768\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 769\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 770\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 771\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 772\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 773\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 774\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 775\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 776\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 777\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 778\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 779\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 780\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 781\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 782\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 783\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 784\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 785\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 786\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 787\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 788\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 789\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 790\n",
      "Train loss: 0.24176885187625885\n",
      "Test loss: 0.2474985122680664\n",
      "----------------------\n",
      "Epoch: 791\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 792\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 793\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 794\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 795\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 796\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 797\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 798\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 799\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 800\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 801\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 802\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 803\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 804\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 805\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 806\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 807\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 808\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 809\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 810\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 811\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 812\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749848246574402\n",
      "----------------------\n",
      "Epoch: 813\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 814\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 815\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 816\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 817\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 818\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 819\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 820\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 821\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 822\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 823\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 824\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 825\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 826\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 827\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 828\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 829\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 830\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 831\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 832\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 833\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 834\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 835\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 836\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 837\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 838\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 839\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 840\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 841\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 842\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 843\n",
      "Train loss: 0.24176882207393646\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 844\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 845\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 846\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 847\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 848\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 849\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 850\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 851\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 852\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 853\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 854\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 855\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 856\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 857\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 858\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 859\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 860\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 861\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 862\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 863\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 864\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 865\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 866\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 867\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 868\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 869\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 870\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 871\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 872\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 873\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 874\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 875\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 876\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 877\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 878\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 879\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 880\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 881\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 882\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 883\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 884\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 885\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 886\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 887\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 888\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 889\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 890\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 891\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 892\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 893\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 894\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 895\n",
      "Train loss: 0.24176880717277527\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 896\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 897\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 898\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 899\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 900\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 901\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 902\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 903\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 904\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 905\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 906\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 907\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 908\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 909\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 910\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 911\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 912\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 913\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 914\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 915\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 916\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 917\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749846756458282\n",
      "----------------------\n",
      "Epoch: 918\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 919\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 920\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 921\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 922\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 923\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 924\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 925\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 926\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 927\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 928\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 929\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 930\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 931\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 932\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 933\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 934\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 935\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 936\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 937\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 938\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 939\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 940\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 941\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 942\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 943\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 944\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 945\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 946\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 947\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 948\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 949\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 950\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 951\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 952\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 953\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 954\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 955\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 956\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 957\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 958\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 959\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 960\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 961\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 962\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 963\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 964\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 965\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 966\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 967\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 968\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 969\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 970\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 971\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 972\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 973\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 974\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 975\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 976\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 977\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 978\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 979\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 980\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 981\n",
      "Train loss: 0.24176877737045288\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 982\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 983\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749843776226044\n",
      "----------------------\n",
      "Epoch: 984\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 985\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 986\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 987\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 988\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 989\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 990\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 991\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 992\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 993\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 994\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 995\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 996\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 997\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 998\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 999\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1000\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1001\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1002\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1003\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1004\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1005\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1006\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1007\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1008\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1009\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1010\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1011\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1012\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1013\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1014\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1015\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1016\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1017\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1018\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1019\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1020\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749839305877686\n",
      "----------------------\n",
      "Epoch: 1021\n",
      "Train loss: 0.2417687624692917\n",
      "Test loss: 0.24749839305877686\n",
      "----------------------\n",
      "Epoch: 1022\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1023\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1024\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1025\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1026\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1027\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1028\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1029\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1030\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1031\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1032\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1033\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1034\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1035\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1036\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1037\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1038\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749839305877686\n",
      "----------------------\n",
      "Epoch: 1039\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749839305877686\n",
      "----------------------\n",
      "Epoch: 1040\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749839305877686\n",
      "----------------------\n",
      "Epoch: 1041\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1042\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1043\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749839305877686\n",
      "----------------------\n",
      "Epoch: 1044\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749839305877686\n",
      "----------------------\n",
      "Epoch: 1045\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1046\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1047\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749839305877686\n",
      "----------------------\n",
      "Epoch: 1048\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749839305877686\n",
      "----------------------\n",
      "Epoch: 1049\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749839305877686\n",
      "----------------------\n",
      "Epoch: 1050\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1051\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1052\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749839305877686\n",
      "----------------------\n",
      "Epoch: 1053\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1054\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1055\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749842286109924\n",
      "----------------------\n",
      "Epoch: 1056\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749839305877686\n",
      "----------------------\n",
      "Epoch: 1057\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749839305877686\n",
      "----------------------\n",
      "Epoch: 1058\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749839305877686\n",
      "----------------------\n",
      "Epoch: 1059\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749839305877686\n",
      "----------------------\n",
      "Epoch: 1060\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749839305877686\n",
      "----------------------\n",
      "Epoch: 1061\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749839305877686\n",
      "----------------------\n",
      "Epoch: 1062\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749839305877686\n",
      "----------------------\n",
      "Epoch: 1063\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749839305877686\n",
      "----------------------\n",
      "Epoch: 1064\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749839305877686\n",
      "----------------------\n",
      "Epoch: 1065\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749839305877686\n",
      "----------------------\n",
      "Epoch: 1066\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749839305877686\n",
      "----------------------\n",
      "Epoch: 1067\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749839305877686\n",
      "----------------------\n",
      "Epoch: 1068\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749839305877686\n",
      "----------------------\n",
      "Epoch: 1069\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749839305877686\n",
      "----------------------\n",
      "Epoch: 1070\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749839305877686\n",
      "----------------------\n",
      "Epoch: 1071\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749839305877686\n",
      "----------------------\n",
      "Epoch: 1072\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1073\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1074\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1075\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1076\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1077\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1078\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1079\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1080\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1081\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1082\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1083\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1084\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1085\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1086\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1087\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1088\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1089\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1090\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1091\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1092\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1093\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1094\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1095\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1096\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1097\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1098\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1099\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1100\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1101\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1102\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1103\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1104\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1105\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1106\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1107\n",
      "Train loss: 0.2417687326669693\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1108\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1109\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1110\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1111\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1112\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1113\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1114\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1115\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1116\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1117\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1118\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1119\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1120\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1121\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1122\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1123\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1124\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1125\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1126\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1127\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1128\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1129\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1130\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1131\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1132\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1133\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1134\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1135\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1136\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1137\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1138\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1139\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1140\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1141\n",
      "Train loss: 0.2417687177658081\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1142\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1143\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1144\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1145\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1146\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1147\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1148\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1149\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1150\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1151\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1152\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1153\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1154\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1155\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1156\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1157\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1158\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1159\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1160\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1161\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1162\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1163\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1164\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1165\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1166\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1167\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1168\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1169\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1170\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1171\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1172\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1173\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1174\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1175\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1176\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1177\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1178\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1179\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1180\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1181\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1182\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1183\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1184\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749837815761566\n",
      "----------------------\n",
      "Epoch: 1185\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1186\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1187\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1188\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1189\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1190\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1191\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1192\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1193\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1194\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1195\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1196\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1197\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1198\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1199\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1200\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1201\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1202\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1203\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1204\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1205\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1206\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1207\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1208\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1209\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1210\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1211\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1212\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1213\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1214\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1215\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1216\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1217\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1218\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1219\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1220\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1221\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1222\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1223\n",
      "Train loss: 0.2417687028646469\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1224\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1225\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1226\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749836325645447\n",
      "----------------------\n",
      "Epoch: 1227\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1228\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1229\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1230\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1231\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1232\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1233\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1234\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1235\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1236\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1237\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1238\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1239\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1240\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1241\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1242\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1243\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1244\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1245\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1246\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1247\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1248\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1249\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1250\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1251\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1252\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1253\n",
      "Train loss: 0.24176867306232452\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1254\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1255\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1256\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1257\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1258\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.2474983185529709\n",
      "----------------------\n",
      "Epoch: 1259\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1260\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1261\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1262\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1263\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1264\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1265\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1266\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1267\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1268\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1269\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1270\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1271\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1272\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1273\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1274\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1275\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1276\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1277\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1278\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1279\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1280\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1281\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1282\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1283\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1284\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1285\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1286\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1287\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1288\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1289\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1290\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1291\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1292\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1293\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1294\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1295\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1296\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1297\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1298\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1299\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1300\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1301\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1302\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1303\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.2474983185529709\n",
      "----------------------\n",
      "Epoch: 1304\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.2474983185529709\n",
      "----------------------\n",
      "Epoch: 1305\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1306\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1307\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1308\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1309\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1310\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1311\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1312\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1313\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1314\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1315\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.2474983185529709\n",
      "----------------------\n",
      "Epoch: 1316\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.2474983185529709\n",
      "----------------------\n",
      "Epoch: 1317\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1318\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.2474983185529709\n",
      "----------------------\n",
      "Epoch: 1319\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.2474983185529709\n",
      "----------------------\n",
      "Epoch: 1320\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.2474983185529709\n",
      "----------------------\n",
      "Epoch: 1321\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.2474983185529709\n",
      "----------------------\n",
      "Epoch: 1322\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474983185529709\n",
      "----------------------\n",
      "Epoch: 1323\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.2474983185529709\n",
      "----------------------\n",
      "Epoch: 1324\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474983185529709\n",
      "----------------------\n",
      "Epoch: 1325\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1326\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474983185529709\n",
      "----------------------\n",
      "Epoch: 1327\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474983185529709\n",
      "----------------------\n",
      "Epoch: 1328\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474983185529709\n",
      "----------------------\n",
      "Epoch: 1329\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1330\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1331\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1332\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1333\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1334\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1335\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474983185529709\n",
      "----------------------\n",
      "Epoch: 1336\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.2474983185529709\n",
      "----------------------\n",
      "Epoch: 1337\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1338\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474983185529709\n",
      "----------------------\n",
      "Epoch: 1339\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.24749833345413208\n",
      "----------------------\n",
      "Epoch: 1340\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474983185529709\n",
      "----------------------\n",
      "Epoch: 1341\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1342\n",
      "Train loss: 0.24176865816116333\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1343\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474983185529709\n",
      "----------------------\n",
      "Epoch: 1344\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1345\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1346\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1347\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1348\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1349\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1350\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1351\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1352\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1353\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1354\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1355\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1356\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1357\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1358\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1359\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1360\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1361\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1362\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1363\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1364\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1365\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1366\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1367\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1368\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1369\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1370\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1371\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1372\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1373\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1374\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1375\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1376\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1377\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1378\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1379\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1380\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1381\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1382\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1383\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1384\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1385\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1386\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1387\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1388\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1389\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1390\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1391\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1392\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1393\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1394\n",
      "Train loss: 0.24176862835884094\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1395\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1396\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1397\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1398\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1399\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1400\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1401\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1402\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1403\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1404\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1405\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1406\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1407\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1408\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1409\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1410\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1411\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1412\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1413\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1414\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1415\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1416\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1417\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1418\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1419\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1420\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1421\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1422\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1423\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1424\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1425\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1426\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1427\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1428\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1429\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1430\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1431\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1432\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1433\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1434\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1435\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1436\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982738494873\n",
      "----------------------\n",
      "Epoch: 1437\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982738494873\n",
      "----------------------\n",
      "Epoch: 1438\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1439\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1440\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982738494873\n",
      "----------------------\n",
      "Epoch: 1441\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1442\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1443\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1444\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1445\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1446\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1447\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1448\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1449\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1450\n",
      "Train loss: 0.24176861345767975\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1451\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1452\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1453\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1454\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1455\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982738494873\n",
      "----------------------\n",
      "Epoch: 1456\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1457\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982738494873\n",
      "----------------------\n",
      "Epoch: 1458\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982738494873\n",
      "----------------------\n",
      "Epoch: 1459\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982738494873\n",
      "----------------------\n",
      "Epoch: 1460\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982738494873\n",
      "----------------------\n",
      "Epoch: 1461\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1462\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982738494873\n",
      "----------------------\n",
      "Epoch: 1463\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982738494873\n",
      "----------------------\n",
      "Epoch: 1464\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982738494873\n",
      "----------------------\n",
      "Epoch: 1465\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982738494873\n",
      "----------------------\n",
      "Epoch: 1466\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982738494873\n",
      "----------------------\n",
      "Epoch: 1467\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982738494873\n",
      "----------------------\n",
      "Epoch: 1468\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1469\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1470\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982738494873\n",
      "----------------------\n",
      "Epoch: 1471\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982738494873\n",
      "----------------------\n",
      "Epoch: 1472\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982738494873\n",
      "----------------------\n",
      "Epoch: 1473\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982738494873\n",
      "----------------------\n",
      "Epoch: 1474\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982738494873\n",
      "----------------------\n",
      "Epoch: 1475\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982738494873\n",
      "----------------------\n",
      "Epoch: 1476\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982738494873\n",
      "----------------------\n",
      "Epoch: 1477\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1478\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1479\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1480\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1481\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982738494873\n",
      "----------------------\n",
      "Epoch: 1482\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982738494873\n",
      "----------------------\n",
      "Epoch: 1483\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982887506485\n",
      "----------------------\n",
      "Epoch: 1484\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982738494873\n",
      "----------------------\n",
      "Epoch: 1485\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1486\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1487\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1488\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1489\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1490\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1491\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1492\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1493\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1494\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1495\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1496\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1497\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1498\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1499\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1500\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1501\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1502\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1503\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1504\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1505\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.2474982738494873\n",
      "----------------------\n",
      "Epoch: 1506\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1507\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1508\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1509\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1510\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1511\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1512\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1513\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1514\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1515\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1516\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1517\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1518\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1519\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1520\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1521\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1522\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1523\n",
      "Train loss: 0.24176858365535736\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1524\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1525\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1526\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1527\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1528\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1529\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1530\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1531\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1532\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1533\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1534\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1535\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1536\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1537\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1538\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1539\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1540\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1541\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1542\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1543\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1544\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1545\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1546\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1547\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1548\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1549\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1550\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1551\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1552\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1553\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1554\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1555\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1556\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1557\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1558\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1559\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1560\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1561\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1562\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1563\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1564\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1565\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1566\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1567\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1568\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1569\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1570\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1571\n",
      "Train loss: 0.24176856875419617\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1572\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1573\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1574\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749822914600372\n",
      "----------------------\n",
      "Epoch: 1575\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1576\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749822914600372\n",
      "----------------------\n",
      "Epoch: 1577\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749822914600372\n",
      "----------------------\n",
      "Epoch: 1578\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749822914600372\n",
      "----------------------\n",
      "Epoch: 1579\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1580\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1581\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1582\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749822914600372\n",
      "----------------------\n",
      "Epoch: 1583\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1584\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1585\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749822914600372\n",
      "----------------------\n",
      "Epoch: 1586\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1587\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1588\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749822914600372\n",
      "----------------------\n",
      "Epoch: 1589\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1590\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749822914600372\n",
      "----------------------\n",
      "Epoch: 1591\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1592\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1593\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1594\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1595\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1596\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749824404716492\n",
      "----------------------\n",
      "Epoch: 1597\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749822914600372\n",
      "----------------------\n",
      "Epoch: 1598\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749822914600372\n",
      "----------------------\n",
      "Epoch: 1599\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749822914600372\n",
      "----------------------\n",
      "Epoch: 1600\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749822914600372\n",
      "----------------------\n",
      "Epoch: 1601\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1602\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749822914600372\n",
      "----------------------\n",
      "Epoch: 1603\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749822914600372\n",
      "----------------------\n",
      "Epoch: 1604\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749822914600372\n",
      "----------------------\n",
      "Epoch: 1605\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749822914600372\n",
      "----------------------\n",
      "Epoch: 1606\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1607\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749822914600372\n",
      "----------------------\n",
      "Epoch: 1608\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1609\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1610\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1611\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1612\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1613\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1614\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1615\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749822914600372\n",
      "----------------------\n",
      "Epoch: 1616\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749822914600372\n",
      "----------------------\n",
      "Epoch: 1617\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749822914600372\n",
      "----------------------\n",
      "Epoch: 1618\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1619\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1620\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749822914600372\n",
      "----------------------\n",
      "Epoch: 1621\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1622\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1623\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1624\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1625\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1626\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1627\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1628\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1629\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1630\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1631\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1632\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1633\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1634\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1635\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1636\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1637\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1638\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1639\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1640\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1641\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1642\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1643\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1644\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1645\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1646\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1647\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1648\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1649\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1650\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1651\n",
      "Train loss: 0.24176855385303497\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1652\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1653\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1654\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1655\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1656\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1657\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1658\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1659\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1660\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1661\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1662\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1663\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1664\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1665\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1666\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1667\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1668\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1669\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1670\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1671\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1672\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1673\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1674\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1675\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1676\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1677\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1678\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1679\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1680\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1681\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1682\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1683\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1684\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1685\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1686\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1687\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1688\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1689\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1690\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1691\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1692\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1693\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1694\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1695\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1696\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1697\n",
      "Train loss: 0.24176852405071259\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1698\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1699\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1700\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1701\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1702\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1703\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1704\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1705\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1706\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749821424484253\n",
      "----------------------\n",
      "Epoch: 1707\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1708\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1709\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1710\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1711\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1712\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1713\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1714\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1715\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1716\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1717\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1718\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1719\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1720\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1721\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1722\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1723\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1724\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1725\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1726\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1727\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1728\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1729\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1730\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1731\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1732\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1733\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1734\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1735\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1736\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1737\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1738\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1739\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1740\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1741\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1742\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1743\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749818444252014\n",
      "----------------------\n",
      "Epoch: 1744\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1745\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1746\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1747\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1748\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1749\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1750\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1751\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1752\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1753\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1754\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1755\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1756\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1757\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1758\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1759\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1760\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1761\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1762\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1763\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1764\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1765\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1766\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1767\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1768\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1769\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1770\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1771\n",
      "Train loss: 0.2417685091495514\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1772\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1773\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1774\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1775\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1776\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1777\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1778\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1779\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1780\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1781\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1782\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1783\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1784\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1785\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1786\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1787\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1788\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1789\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1790\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1791\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1792\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1793\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1794\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1795\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1796\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1797\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1798\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1799\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1800\n",
      "Train loss: 0.241768479347229\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1801\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1802\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1803\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1804\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1805\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1806\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1807\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1808\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1809\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1810\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1811\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1812\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1813\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1814\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1815\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1816\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1817\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1818\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1819\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1820\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1821\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1822\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1823\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1824\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1825\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1826\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1827\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1828\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1829\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1830\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1831\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1832\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1833\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1834\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1835\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1836\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1837\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1838\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1839\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1840\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1841\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1842\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1843\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1844\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1845\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1846\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1847\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1848\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1849\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1850\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1851\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749813973903656\n",
      "----------------------\n",
      "Epoch: 1852\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749813973903656\n",
      "----------------------\n",
      "Epoch: 1853\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749813973903656\n",
      "----------------------\n",
      "Epoch: 1854\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749813973903656\n",
      "----------------------\n",
      "Epoch: 1855\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1856\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749813973903656\n",
      "----------------------\n",
      "Epoch: 1857\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1858\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1859\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749813973903656\n",
      "----------------------\n",
      "Epoch: 1860\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1861\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1862\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749816954135895\n",
      "----------------------\n",
      "Epoch: 1863\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749813973903656\n",
      "----------------------\n",
      "Epoch: 1864\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1865\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1866\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1867\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1868\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1869\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1870\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1871\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1872\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1873\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1874\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1875\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749813973903656\n",
      "----------------------\n",
      "Epoch: 1876\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749813973903656\n",
      "----------------------\n",
      "Epoch: 1877\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1878\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1879\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1880\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749813973903656\n",
      "----------------------\n",
      "Epoch: 1881\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1882\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749813973903656\n",
      "----------------------\n",
      "Epoch: 1883\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749813973903656\n",
      "----------------------\n",
      "Epoch: 1884\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1885\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1886\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1887\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1888\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1889\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749813973903656\n",
      "----------------------\n",
      "Epoch: 1890\n",
      "Train loss: 0.2417684644460678\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1891\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1892\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1893\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1894\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1895\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1896\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1897\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1898\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1899\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1900\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1901\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1902\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1903\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1904\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1905\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1906\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1907\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1908\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1909\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1910\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1911\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1912\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1913\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1914\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1915\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1916\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1917\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1918\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1919\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1920\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1921\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1922\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1923\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1924\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1925\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1926\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1927\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1928\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1929\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1930\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1931\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1932\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1933\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1934\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1935\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1936\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1937\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1938\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1939\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1940\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1941\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1942\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1943\n",
      "Train loss: 0.24176843464374542\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1944\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1945\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1946\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1947\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1948\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1949\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1950\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1951\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1952\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1953\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1954\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1955\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1956\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1957\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1958\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1959\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1960\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1961\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1962\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1963\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1964\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1965\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1966\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1967\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1968\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1969\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1970\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1971\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1972\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1973\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1974\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1975\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1976\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1977\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1978\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1979\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1980\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1981\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1982\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1983\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1984\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1985\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1986\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1987\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1988\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1989\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1990\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1991\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1992\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1993\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 1994\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749809503555298\n",
      "----------------------\n",
      "Epoch: 1995\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749809503555298\n",
      "----------------------\n",
      "Epoch: 1996\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749809503555298\n",
      "----------------------\n",
      "Epoch: 1997\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749809503555298\n",
      "----------------------\n",
      "Epoch: 1998\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749809503555298\n",
      "----------------------\n",
      "Epoch: 1999\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 2000\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 2001\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 2002\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 2003\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 2004\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 2005\n",
      "Train loss: 0.24176841974258423\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 2006\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749809503555298\n",
      "----------------------\n",
      "Epoch: 2007\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 2008\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 2009\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 2010\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749809503555298\n",
      "----------------------\n",
      "Epoch: 2011\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749809503555298\n",
      "----------------------\n",
      "Epoch: 2012\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749809503555298\n",
      "----------------------\n",
      "Epoch: 2013\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 2014\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 2015\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749809503555298\n",
      "----------------------\n",
      "Epoch: 2016\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 2017\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 2018\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749809503555298\n",
      "----------------------\n",
      "Epoch: 2019\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749809503555298\n",
      "----------------------\n",
      "Epoch: 2020\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2021\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749809503555298\n",
      "----------------------\n",
      "Epoch: 2022\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2023\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2024\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 2025\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2026\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2027\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2028\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2029\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2030\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2031\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 2032\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749809503555298\n",
      "----------------------\n",
      "Epoch: 2033\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749812483787537\n",
      "----------------------\n",
      "Epoch: 2034\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749809503555298\n",
      "----------------------\n",
      "Epoch: 2035\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749809503555298\n",
      "----------------------\n",
      "Epoch: 2036\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749809503555298\n",
      "----------------------\n",
      "Epoch: 2037\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2038\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749809503555298\n",
      "----------------------\n",
      "Epoch: 2039\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749809503555298\n",
      "----------------------\n",
      "Epoch: 2040\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2041\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749809503555298\n",
      "----------------------\n",
      "Epoch: 2042\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2043\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749809503555298\n",
      "----------------------\n",
      "Epoch: 2044\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749809503555298\n",
      "----------------------\n",
      "Epoch: 2045\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2046\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2047\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2048\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2049\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749809503555298\n",
      "----------------------\n",
      "Epoch: 2050\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2051\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2052\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2053\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2054\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2055\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2056\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2057\n",
      "Train loss: 0.24176840484142303\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2058\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2059\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2060\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2061\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2062\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2063\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2064\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2065\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2066\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2067\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2068\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2069\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2070\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2071\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2072\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2073\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2074\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2075\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2076\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2077\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2078\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2079\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2080\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2081\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2082\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2083\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2084\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2085\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2086\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2087\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2088\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2089\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2090\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2091\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2092\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2093\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2094\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2095\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2096\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2097\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2098\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2099\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2100\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2101\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2102\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2103\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2104\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2105\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2106\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2107\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2108\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2109\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2110\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2111\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.24749808013439178\n",
      "----------------------\n",
      "Epoch: 2112\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2113\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2114\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2115\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2116\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2117\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2118\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2119\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2120\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2121\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2122\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2123\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2124\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2125\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2126\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2127\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2128\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2129\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2130\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2131\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2132\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2133\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2134\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2135\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2136\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2137\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2138\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2139\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2140\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2141\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2142\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2143\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2144\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2145\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2146\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2147\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2148\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2149\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2150\n",
      "Train loss: 0.24176837503910065\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2151\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2152\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2153\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2154\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2155\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2156\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2157\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2158\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2159\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2160\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2161\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2162\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2163\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2164\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2165\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2166\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2167\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2168\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980652332306\n",
      "----------------------\n",
      "Epoch: 2169\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2170\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2171\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2172\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2173\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2174\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2175\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2176\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2177\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2178\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2179\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2180\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2181\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2182\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2183\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2184\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2185\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2186\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2187\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2188\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2189\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2190\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2191\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2192\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2193\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2194\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2195\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2196\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2197\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2198\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2199\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2200\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2201\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2202\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2203\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2204\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2205\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2206\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2207\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2208\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2209\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2210\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2211\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2212\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2213\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2214\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2215\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2216\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2217\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2218\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2219\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2220\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2221\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2222\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2223\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2224\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2225\n",
      "Train loss: 0.24176836013793945\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2226\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2227\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2228\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2229\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2230\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2231\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2232\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2233\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2234\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2235\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2236\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2237\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2238\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.247498020529747\n",
      "----------------------\n",
      "Epoch: 2239\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2240\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2241\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2242\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2243\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2244\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2245\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2246\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2247\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2248\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2249\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2250\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2251\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2252\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2253\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2254\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2255\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2256\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2257\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2258\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2259\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.247498020529747\n",
      "----------------------\n",
      "Epoch: 2260\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2261\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.2474980354309082\n",
      "----------------------\n",
      "Epoch: 2262\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2263\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.247498020529747\n",
      "----------------------\n",
      "Epoch: 2264\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2265\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2266\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2267\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2268\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2269\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2270\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2271\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2272\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.247498020529747\n",
      "----------------------\n",
      "Epoch: 2273\n",
      "Train loss: 0.24176833033561707\n",
      "Test loss: 0.247498020529747\n",
      "----------------------\n",
      "Epoch: 2274\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.247498020529747\n",
      "----------------------\n",
      "Epoch: 2275\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2276\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2277\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2278\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2279\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2280\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2281\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2282\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2283\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2284\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2285\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2286\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2287\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2288\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2289\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2290\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2291\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.247498020529747\n",
      "----------------------\n",
      "Epoch: 2292\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2293\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2294\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2295\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2296\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2297\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2298\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2299\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2300\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2301\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2302\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2303\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2304\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2305\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2306\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2307\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2308\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2309\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2310\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2311\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2312\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2313\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2314\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2315\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2316\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2317\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2318\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2319\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2320\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2321\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2322\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2323\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2324\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2325\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2326\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2327\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2328\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2329\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2330\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2331\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2332\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2333\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2334\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2335\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2336\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2337\n",
      "Train loss: 0.24176831543445587\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2338\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2339\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2340\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2341\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2342\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2343\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2344\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2345\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2346\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2347\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2348\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2349\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2350\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2351\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2352\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2353\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2354\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2355\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2356\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2357\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2358\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2359\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2360\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2361\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2362\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2363\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2364\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2365\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2366\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2367\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2368\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2369\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2370\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2371\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2372\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2373\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2374\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2375\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2376\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2377\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2378\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2379\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2380\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2381\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2382\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2383\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2384\n",
      "Train loss: 0.24176828563213348\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2385\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2386\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2387\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2388\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2389\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2390\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2391\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2392\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749797582626343\n",
      "----------------------\n",
      "Epoch: 2393\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2394\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2395\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749797582626343\n",
      "----------------------\n",
      "Epoch: 2396\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2397\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749797582626343\n",
      "----------------------\n",
      "Epoch: 2398\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2399\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2400\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749797582626343\n",
      "----------------------\n",
      "Epoch: 2401\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2402\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2403\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2404\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2405\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2406\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2407\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2408\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749797582626343\n",
      "----------------------\n",
      "Epoch: 2409\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2410\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749797582626343\n",
      "----------------------\n",
      "Epoch: 2411\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749799072742462\n",
      "----------------------\n",
      "Epoch: 2412\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2413\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2414\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2415\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2416\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2417\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2418\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2419\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2420\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2421\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749797582626343\n",
      "----------------------\n",
      "Epoch: 2422\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2423\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2424\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2425\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749797582626343\n",
      "----------------------\n",
      "Epoch: 2426\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2427\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2428\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2429\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2430\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2431\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2432\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2433\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2434\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2435\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2436\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2437\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2438\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2439\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2440\n",
      "Train loss: 0.2417682707309723\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2441\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2442\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2443\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2444\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2445\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2446\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2447\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2448\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2449\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2450\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2451\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2452\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2453\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2454\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2455\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2456\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2457\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2458\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2459\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2460\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2461\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2462\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2463\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2464\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2465\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2466\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2467\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2468\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2469\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2470\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2471\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2472\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2473\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2474\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2475\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2476\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2477\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2478\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2479\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2480\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2481\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2482\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2483\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2484\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2485\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2486\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2487\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2488\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2489\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2490\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2491\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2492\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2493\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2494\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2495\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2496\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2497\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2498\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2499\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2500\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2501\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2502\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2503\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2504\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2505\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2506\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2507\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2508\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2509\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2510\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2511\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2512\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2513\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2514\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2515\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2516\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2517\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2518\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2519\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2520\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2521\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2522\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749793112277985\n",
      "----------------------\n",
      "Epoch: 2523\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749793112277985\n",
      "----------------------\n",
      "Epoch: 2524\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749793112277985\n",
      "----------------------\n",
      "Epoch: 2525\n",
      "Train loss: 0.2417682558298111\n",
      "Test loss: 0.24749793112277985\n",
      "----------------------\n",
      "Epoch: 2526\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2527\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2528\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2529\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2530\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2531\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2532\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749793112277985\n",
      "----------------------\n",
      "Epoch: 2533\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749793112277985\n",
      "----------------------\n",
      "Epoch: 2534\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749793112277985\n",
      "----------------------\n",
      "Epoch: 2535\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749793112277985\n",
      "----------------------\n",
      "Epoch: 2536\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749793112277985\n",
      "----------------------\n",
      "Epoch: 2537\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749793112277985\n",
      "----------------------\n",
      "Epoch: 2538\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749793112277985\n",
      "----------------------\n",
      "Epoch: 2539\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749793112277985\n",
      "----------------------\n",
      "Epoch: 2540\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749793112277985\n",
      "----------------------\n",
      "Epoch: 2541\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2542\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2543\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749793112277985\n",
      "----------------------\n",
      "Epoch: 2544\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749793112277985\n",
      "----------------------\n",
      "Epoch: 2545\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749793112277985\n",
      "----------------------\n",
      "Epoch: 2546\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749793112277985\n",
      "----------------------\n",
      "Epoch: 2547\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749793112277985\n",
      "----------------------\n",
      "Epoch: 2548\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2549\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749794602394104\n",
      "----------------------\n",
      "Epoch: 2550\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2551\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749793112277985\n",
      "----------------------\n",
      "Epoch: 2552\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2553\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2554\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2555\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2556\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749793112277985\n",
      "----------------------\n",
      "Epoch: 2557\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2558\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2559\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2560\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749793112277985\n",
      "----------------------\n",
      "Epoch: 2561\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749793112277985\n",
      "----------------------\n",
      "Epoch: 2562\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749793112277985\n",
      "----------------------\n",
      "Epoch: 2563\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749793112277985\n",
      "----------------------\n",
      "Epoch: 2564\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2565\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2566\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2567\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2568\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2569\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2570\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2571\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2572\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2573\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2574\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2575\n",
      "Train loss: 0.2417682260274887\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2576\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2577\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2578\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2579\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2580\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2581\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2582\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2583\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2584\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2585\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2586\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2587\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2588\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2589\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2590\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2591\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2592\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2593\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2594\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2595\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2596\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2597\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2598\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2599\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2600\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2601\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2602\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2603\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2604\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2605\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2606\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2607\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2608\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2609\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2610\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2611\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2612\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2613\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2614\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2615\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2616\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2617\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2618\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2619\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2620\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2621\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2622\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2623\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2624\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2625\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2626\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2627\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2628\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2629\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2630\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2631\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2632\n",
      "Train loss: 0.24176821112632751\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2633\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2634\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2635\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2636\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2637\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2638\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2639\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2640\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749788641929626\n",
      "----------------------\n",
      "Epoch: 2641\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749788641929626\n",
      "----------------------\n",
      "Epoch: 2642\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2643\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2644\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749788641929626\n",
      "----------------------\n",
      "Epoch: 2645\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749788641929626\n",
      "----------------------\n",
      "Epoch: 2646\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2647\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2648\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749788641929626\n",
      "----------------------\n",
      "Epoch: 2649\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2650\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749788641929626\n",
      "----------------------\n",
      "Epoch: 2651\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749788641929626\n",
      "----------------------\n",
      "Epoch: 2652\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2653\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2654\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2655\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2656\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2657\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2658\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2659\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2660\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2661\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2662\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749788641929626\n",
      "----------------------\n",
      "Epoch: 2663\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2664\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2665\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749788641929626\n",
      "----------------------\n",
      "Epoch: 2666\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2667\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749791622161865\n",
      "----------------------\n",
      "Epoch: 2668\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749788641929626\n",
      "----------------------\n",
      "Epoch: 2669\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749788641929626\n",
      "----------------------\n",
      "Epoch: 2670\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749788641929626\n",
      "----------------------\n",
      "Epoch: 2671\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2672\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749788641929626\n",
      "----------------------\n",
      "Epoch: 2673\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749788641929626\n",
      "----------------------\n",
      "Epoch: 2674\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749788641929626\n",
      "----------------------\n",
      "Epoch: 2675\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749788641929626\n",
      "----------------------\n",
      "Epoch: 2676\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749788641929626\n",
      "----------------------\n",
      "Epoch: 2677\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749788641929626\n",
      "----------------------\n",
      "Epoch: 2678\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749788641929626\n",
      "----------------------\n",
      "Epoch: 2679\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749788641929626\n",
      "----------------------\n",
      "Epoch: 2680\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749788641929626\n",
      "----------------------\n",
      "Epoch: 2681\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749788641929626\n",
      "----------------------\n",
      "Epoch: 2682\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749788641929626\n",
      "----------------------\n",
      "Epoch: 2683\n",
      "Train loss: 0.24176818132400513\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2684\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2685\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2686\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2687\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2688\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2689\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2690\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2691\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2692\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2693\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2694\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2695\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2696\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2697\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2698\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2699\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2700\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2701\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2702\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2703\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2704\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2705\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749788641929626\n",
      "----------------------\n",
      "Epoch: 2706\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2707\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2708\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2709\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2710\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2711\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2712\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2713\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2714\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2715\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2716\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2717\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2718\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2719\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2720\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2721\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2722\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2723\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2724\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2725\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2726\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2727\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2728\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2729\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2730\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2731\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2732\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2733\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2734\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2735\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2736\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2737\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2738\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2739\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2740\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2741\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2742\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2743\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2744\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2745\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2746\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2747\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2748\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2749\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2750\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2751\n",
      "Train loss: 0.24176816642284393\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2752\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2753\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2754\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2755\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2756\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2757\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2758\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2759\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2760\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2761\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2762\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2763\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2764\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2765\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2766\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2767\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2768\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2769\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2770\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2771\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2772\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2773\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2774\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2775\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2776\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2777\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2778\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2779\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2780\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2781\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749784171581268\n",
      "----------------------\n",
      "Epoch: 2782\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2783\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749784171581268\n",
      "----------------------\n",
      "Epoch: 2784\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749784171581268\n",
      "----------------------\n",
      "Epoch: 2785\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2786\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749784171581268\n",
      "----------------------\n",
      "Epoch: 2787\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2788\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749784171581268\n",
      "----------------------\n",
      "Epoch: 2789\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2790\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2791\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2792\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2793\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2794\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2795\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749784171581268\n",
      "----------------------\n",
      "Epoch: 2796\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749784171581268\n",
      "----------------------\n",
      "Epoch: 2797\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2798\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2799\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2800\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2801\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749787151813507\n",
      "----------------------\n",
      "Epoch: 2802\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749784171581268\n",
      "----------------------\n",
      "Epoch: 2803\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749784171581268\n",
      "----------------------\n",
      "Epoch: 2804\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749784171581268\n",
      "----------------------\n",
      "Epoch: 2805\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749784171581268\n",
      "----------------------\n",
      "Epoch: 2806\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2807\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2808\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2809\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749784171581268\n",
      "----------------------\n",
      "Epoch: 2810\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749784171581268\n",
      "----------------------\n",
      "Epoch: 2811\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749784171581268\n",
      "----------------------\n",
      "Epoch: 2812\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749784171581268\n",
      "----------------------\n",
      "Epoch: 2813\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749784171581268\n",
      "----------------------\n",
      "Epoch: 2814\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749784171581268\n",
      "----------------------\n",
      "Epoch: 2815\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2816\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2817\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749784171581268\n",
      "----------------------\n",
      "Epoch: 2818\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2819\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2820\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.24749784171581268\n",
      "----------------------\n",
      "Epoch: 2821\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2822\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.24749784171581268\n",
      "----------------------\n",
      "Epoch: 2823\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2824\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2825\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2826\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2827\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2828\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2829\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2830\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2831\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2832\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2833\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2834\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2835\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2836\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2837\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2838\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2839\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2840\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2841\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2842\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2843\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2844\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2845\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2846\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2847\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2848\n",
      "Train loss: 0.24176813662052155\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2849\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2850\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2851\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2852\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2853\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2854\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2855\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2856\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2857\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2858\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2859\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2860\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2861\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2862\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2863\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2864\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2865\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2866\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2867\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2868\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2869\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2870\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2871\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2872\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2873\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2874\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2875\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2876\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2877\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2878\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2879\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2880\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2881\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2882\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2883\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2884\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2885\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2886\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2887\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2888\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2889\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2890\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2891\n",
      "Train loss: 0.24176812171936035\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2892\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2893\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2894\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2895\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2896\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2897\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2898\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2899\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2900\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2901\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2902\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2903\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2904\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2905\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2906\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2907\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2908\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2909\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2910\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2911\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2912\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2913\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2914\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2915\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2916\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2917\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2918\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2919\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2920\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2921\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2922\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2923\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2924\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2925\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2926\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2927\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2928\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2929\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2930\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2931\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2932\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2933\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2934\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2935\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2936\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2937\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2938\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2939\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2940\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2941\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2942\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2943\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2944\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2945\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2946\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2947\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2948\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2949\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2950\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2951\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2952\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2953\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474978268146515\n",
      "----------------------\n",
      "Epoch: 2954\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2955\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2956\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2957\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2958\n",
      "Train loss: 0.24176810681819916\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2959\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2960\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2961\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2962\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2963\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2964\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2965\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2966\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2967\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2968\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2969\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2970\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2971\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2972\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2973\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2974\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977970123291\n",
      "----------------------\n",
      "Epoch: 2975\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2976\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2977\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2978\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2979\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2980\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2981\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2982\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2983\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2984\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2985\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2986\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2987\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2988\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2989\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2990\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2991\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2992\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2993\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2994\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2995\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2996\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2997\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2998\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 2999\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3000\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3001\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3002\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3003\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3004\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3005\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3006\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3007\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3008\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3009\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3010\n",
      "Train loss: 0.24176807701587677\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3011\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3012\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3013\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3014\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3015\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3016\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3017\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3018\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3019\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3020\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3021\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3022\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3023\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3024\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3025\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3026\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3027\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3028\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3029\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3030\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3031\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3032\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3033\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3034\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3035\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3036\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3037\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3038\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3039\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3040\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3041\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3042\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3043\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3044\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3045\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3046\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3047\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3048\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3049\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3050\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3051\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3052\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3053\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3054\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3055\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3056\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3057\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3058\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3059\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3060\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3061\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3062\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3063\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3064\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3065\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3066\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3067\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3068\n",
      "Train loss: 0.2417680323123932\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3069\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3070\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3071\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3072\n",
      "Train loss: 0.2417680323123932\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3073\n",
      "Train loss: 0.2417680323123932\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3074\n",
      "Train loss: 0.2417680323123932\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3075\n",
      "Train loss: 0.2417680323123932\n",
      "Test loss: 0.2474977821111679\n",
      "----------------------\n",
      "Epoch: 3076\n",
      "Train loss: 0.2417680323123932\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3077\n",
      "Train loss: 0.24176806211471558\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3078\n",
      "Train loss: 0.2417680323123932\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3079\n",
      "Train loss: 0.2417680323123932\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3080\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3081\n",
      "Train loss: 0.2417680323123932\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3082\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3083\n",
      "Train loss: 0.2417680323123932\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3084\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3085\n",
      "Train loss: 0.2417680323123932\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3086\n",
      "Train loss: 0.2417680323123932\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3087\n",
      "Train loss: 0.2417680323123932\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3088\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3089\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.2474977672100067\n",
      "----------------------\n",
      "Epoch: 3090\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3091\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3092\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3093\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3094\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3095\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3096\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3097\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3098\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3099\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3100\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3101\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3102\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3103\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3104\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3105\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3106\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3107\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3108\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3109\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3110\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3111\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3112\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3113\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3114\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3115\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3116\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3117\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3118\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3119\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3120\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3121\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3122\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3123\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3124\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3125\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3126\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3127\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3128\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3129\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3130\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3131\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3132\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3133\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3134\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3135\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3136\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3137\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3138\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3139\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3140\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3141\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3142\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3143\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3144\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3145\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3146\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3147\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3148\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3149\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3150\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3151\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3152\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3153\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3154\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3155\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3156\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3157\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3158\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3159\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3160\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3161\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3162\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3163\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3164\n",
      "Train loss: 0.2417679876089096\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3165\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3166\n",
      "Train loss: 0.2417679876089096\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3167\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3168\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3169\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3170\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3171\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3172\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749772250652313\n",
      "----------------------\n",
      "Epoch: 3173\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3174\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3175\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3176\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3177\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3178\n",
      "Train loss: 0.2417679876089096\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3179\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3180\n",
      "Train loss: 0.2417679876089096\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3181\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3182\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3183\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3184\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3185\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3186\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3187\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3188\n",
      "Train loss: 0.2417679876089096\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3189\n",
      "Train loss: 0.2417679876089096\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3190\n",
      "Train loss: 0.2417679876089096\n",
      "Test loss: 0.24749772250652313\n",
      "----------------------\n",
      "Epoch: 3191\n",
      "Train loss: 0.2417679876089096\n",
      "Test loss: 0.24749772250652313\n",
      "----------------------\n",
      "Epoch: 3192\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3193\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3194\n",
      "Train loss: 0.241768017411232\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3195\n",
      "Train loss: 0.2417679876089096\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3196\n",
      "Train loss: 0.2417679876089096\n",
      "Test loss: 0.24749772250652313\n",
      "----------------------\n",
      "Epoch: 3197\n",
      "Train loss: 0.2417679876089096\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3198\n",
      "Train loss: 0.2417679876089096\n",
      "Test loss: 0.24749773740768433\n",
      "----------------------\n",
      "Epoch: 3199\n",
      "Train loss: 0.2417679876089096\n",
      "Test loss: 0.24749772250652313\n",
      "----------------------\n",
      "Epoch: 3200\n",
      "Train loss: 0.2417679876089096\n",
      "Test loss: 0.24749772250652313\n",
      "----------------------\n",
      "Epoch: 3201\n",
      "Train loss: 0.2417679876089096\n",
      "Test loss: 0.24749772250652313\n",
      "----------------------\n",
      "Epoch: 3202\n",
      "Train loss: 0.2417679876089096\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3203\n",
      "Train loss: 0.2417679876089096\n",
      "Test loss: 0.24749772250652313\n",
      "----------------------\n",
      "Epoch: 3204\n",
      "Train loss: 0.2417679876089096\n",
      "Test loss: 0.24749772250652313\n",
      "----------------------\n",
      "Epoch: 3205\n",
      "Train loss: 0.2417679876089096\n",
      "Test loss: 0.24749772250652313\n",
      "----------------------\n",
      "Epoch: 3206\n",
      "Train loss: 0.2417679876089096\n",
      "Test loss: 0.24749772250652313\n",
      "----------------------\n",
      "Epoch: 3207\n",
      "Train loss: 0.2417679876089096\n",
      "Test loss: 0.24749772250652313\n",
      "----------------------\n",
      "Epoch: 3208\n",
      "Train loss: 0.2417679876089096\n",
      "Test loss: 0.24749772250652313\n",
      "----------------------\n",
      "Epoch: 3209\n",
      "Train loss: 0.2417679876089096\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3210\n",
      "Train loss: 0.2417679876089096\n",
      "Test loss: 0.24749772250652313\n",
      "----------------------\n",
      "Epoch: 3211\n",
      "Train loss: 0.2417679876089096\n",
      "Test loss: 0.24749772250652313\n",
      "----------------------\n",
      "Epoch: 3212\n",
      "Train loss: 0.2417679876089096\n",
      "Test loss: 0.24749772250652313\n",
      "----------------------\n",
      "Epoch: 3213\n",
      "Train loss: 0.2417679876089096\n",
      "Test loss: 0.24749772250652313\n",
      "----------------------\n",
      "Epoch: 3214\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3215\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749772250652313\n",
      "----------------------\n",
      "Epoch: 3216\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749772250652313\n",
      "----------------------\n",
      "Epoch: 3217\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749772250652313\n",
      "----------------------\n",
      "Epoch: 3218\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3219\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3220\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3221\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3222\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3223\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3224\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749772250652313\n",
      "----------------------\n",
      "Epoch: 3225\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3226\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749772250652313\n",
      "----------------------\n",
      "Epoch: 3227\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749772250652313\n",
      "----------------------\n",
      "Epoch: 3228\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3229\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3230\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3231\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3232\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749772250652313\n",
      "----------------------\n",
      "Epoch: 3233\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3234\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3235\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3236\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3237\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3238\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3239\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3240\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3241\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3242\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3243\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3244\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749772250652313\n",
      "----------------------\n",
      "Epoch: 3245\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749772250652313\n",
      "----------------------\n",
      "Epoch: 3246\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3247\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3248\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749772250652313\n",
      "----------------------\n",
      "Epoch: 3249\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3250\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3251\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3252\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3253\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3254\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3255\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3256\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3257\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3258\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3259\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3260\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3261\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3262\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3263\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3264\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3265\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3266\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3267\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3268\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3269\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3270\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3271\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3272\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3273\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3274\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3275\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3276\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3277\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3278\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3279\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3280\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3281\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3282\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3283\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3284\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3285\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3286\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3287\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3288\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3289\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3290\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3291\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3292\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3293\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3294\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749767780303955\n",
      "----------------------\n",
      "Epoch: 3295\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749767780303955\n",
      "----------------------\n",
      "Epoch: 3296\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3297\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3298\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3299\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3300\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3301\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3302\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3303\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3304\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3305\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3306\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3307\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3308\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3309\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3310\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3311\n",
      "Train loss: 0.24176795780658722\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3312\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3313\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3314\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3315\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749767780303955\n",
      "----------------------\n",
      "Epoch: 3316\n",
      "Train loss: 0.24176795780658722\n",
      "Test loss: 0.24749767780303955\n",
      "----------------------\n",
      "Epoch: 3317\n",
      "Train loss: 0.24176795780658722\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3318\n",
      "Train loss: 0.24176795780658722\n",
      "Test loss: 0.24749767780303955\n",
      "----------------------\n",
      "Epoch: 3319\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3320\n",
      "Train loss: 0.24176795780658722\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3321\n",
      "Train loss: 0.24176795780658722\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3322\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3323\n",
      "Train loss: 0.24176795780658722\n",
      "Test loss: 0.24749767780303955\n",
      "----------------------\n",
      "Epoch: 3324\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749767780303955\n",
      "----------------------\n",
      "Epoch: 3325\n",
      "Train loss: 0.24176795780658722\n",
      "Test loss: 0.24749767780303955\n",
      "----------------------\n",
      "Epoch: 3326\n",
      "Train loss: 0.24176795780658722\n",
      "Test loss: 0.24749767780303955\n",
      "----------------------\n",
      "Epoch: 3327\n",
      "Train loss: 0.24176795780658722\n",
      "Test loss: 0.24749767780303955\n",
      "----------------------\n",
      "Epoch: 3328\n",
      "Train loss: 0.24176795780658722\n",
      "Test loss: 0.24749767780303955\n",
      "----------------------\n",
      "Epoch: 3329\n",
      "Train loss: 0.2417679727077484\n",
      "Test loss: 0.24749767780303955\n",
      "----------------------\n",
      "Epoch: 3330\n",
      "Train loss: 0.24176795780658722\n",
      "Test loss: 0.24749769270420074\n",
      "----------------------\n",
      "Epoch: 3331\n",
      "Train loss: 0.24176795780658722\n",
      "Test loss: 0.24749767780303955\n",
      "----------------------\n",
      "Epoch: 3332\n",
      "Train loss: 0.24176795780658722\n",
      "Test loss: 0.24749767780303955\n",
      "----------------------\n",
      "Epoch: 3333\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749767780303955\n",
      "----------------------\n",
      "Epoch: 3334\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749767780303955\n",
      "----------------------\n",
      "Epoch: 3335\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3336\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3337\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3338\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749767780303955\n",
      "----------------------\n",
      "Epoch: 3339\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749767780303955\n",
      "----------------------\n",
      "Epoch: 3340\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749767780303955\n",
      "----------------------\n",
      "Epoch: 3341\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749767780303955\n",
      "----------------------\n",
      "Epoch: 3342\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3343\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749767780303955\n",
      "----------------------\n",
      "Epoch: 3344\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3345\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749767780303955\n",
      "----------------------\n",
      "Epoch: 3346\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3347\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3348\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3349\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3350\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3351\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3352\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3353\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3354\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749767780303955\n",
      "----------------------\n",
      "Epoch: 3355\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749767780303955\n",
      "----------------------\n",
      "Epoch: 3356\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3357\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3358\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3359\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3360\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3361\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3362\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3363\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3364\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3365\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3366\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3367\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3368\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3369\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3370\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3371\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3372\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3373\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3374\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3375\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749767780303955\n",
      "----------------------\n",
      "Epoch: 3376\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3377\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3378\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3379\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3380\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3381\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3382\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3383\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3384\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3385\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3386\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3387\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3388\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3389\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3390\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3391\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3392\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3393\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3394\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3395\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3396\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3397\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3398\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3399\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3400\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3401\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3402\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3403\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3404\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3405\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3406\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3407\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3408\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3409\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3410\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3411\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3412\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3413\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3414\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3415\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3416\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3417\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3418\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3419\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3420\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3421\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3422\n",
      "Train loss: 0.24176791310310364\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3423\n",
      "Train loss: 0.24176791310310364\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3424\n",
      "Train loss: 0.24176791310310364\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3425\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3426\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3427\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3428\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3429\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3430\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3431\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3432\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3433\n",
      "Train loss: 0.24176791310310364\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3434\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3435\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3436\n",
      "Train loss: 0.24176791310310364\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3437\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3438\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3439\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3440\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3441\n",
      "Train loss: 0.24176791310310364\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3442\n",
      "Train loss: 0.24176791310310364\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3443\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3444\n",
      "Train loss: 0.24176791310310364\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3445\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3446\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3447\n",
      "Train loss: 0.24176791310310364\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3448\n",
      "Train loss: 0.24176792800426483\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3449\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3450\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3451\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3452\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3453\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3454\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3455\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3456\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3457\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3458\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3459\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3460\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3461\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3462\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3463\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3464\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3465\n",
      "Train loss: 0.24176791310310364\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3466\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3467\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3468\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3469\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3470\n",
      "Train loss: 0.24176791310310364\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3471\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3472\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3473\n",
      "Train loss: 0.24176791310310364\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3474\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3475\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3476\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3477\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3478\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3479\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3480\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3481\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3482\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3483\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3484\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3485\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3486\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3487\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3488\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3489\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3490\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3491\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3492\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3493\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3494\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3495\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3496\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3497\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3498\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3499\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3500\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3501\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3502\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3503\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3504\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749764800071716\n",
      "----------------------\n",
      "Epoch: 3505\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3506\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3507\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3508\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3509\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3510\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3511\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3512\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3513\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3514\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749763309955597\n",
      "----------------------\n",
      "Epoch: 3515\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3516\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3517\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3518\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3519\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3520\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3521\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3522\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3523\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3524\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3525\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3526\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3527\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3528\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3529\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3530\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3531\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3532\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3533\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3534\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3535\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3536\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3537\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3538\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3539\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3540\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3541\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3542\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3543\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3544\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3545\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3546\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3547\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3548\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3549\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3550\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3551\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3552\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3553\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3554\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3555\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3556\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3557\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3558\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3559\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3560\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3561\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3562\n",
      "Train loss: 0.24176788330078125\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3563\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3564\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3565\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3566\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3567\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3568\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3569\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3570\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3571\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3572\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3573\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3574\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3575\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3576\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3577\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3578\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3579\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3580\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3581\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3582\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3583\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3584\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3585\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3586\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3587\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3588\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3589\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3590\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3591\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3592\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3593\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3594\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3595\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3596\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3597\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3598\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3599\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3600\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3601\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3602\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3603\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3604\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.24749761819839478\n",
      "----------------------\n",
      "Epoch: 3605\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3606\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3607\n",
      "Train loss: 0.24176786839962006\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3608\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3609\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3610\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3611\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3612\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3613\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3614\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3615\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3616\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3617\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3618\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3619\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3620\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3621\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3622\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3623\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3624\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3625\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3626\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3627\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3628\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3629\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3630\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3631\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3632\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3633\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3634\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3635\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3636\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3637\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3638\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3639\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3640\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3641\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3642\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3643\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3644\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3645\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3646\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3647\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3648\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3649\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3650\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3651\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3652\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975883960724\n",
      "----------------------\n",
      "Epoch: 3653\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3654\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3655\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3656\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3657\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3658\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3659\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3660\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3661\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3662\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3663\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3664\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3665\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3666\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3667\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3668\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3669\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3670\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3671\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3672\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3673\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3674\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3675\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3676\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3677\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3678\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3679\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3680\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3681\n",
      "Train loss: 0.24176783859729767\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3682\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3683\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3684\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3685\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3686\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3687\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3688\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3689\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3690\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3691\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3692\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3693\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3694\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3695\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3696\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3697\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3698\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3699\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3700\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3701\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3702\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3703\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3704\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3705\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3706\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3707\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3708\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3709\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3710\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3711\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3712\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3713\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3714\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3715\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3716\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3717\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3718\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3719\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3720\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3721\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3722\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3723\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3724\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3725\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3726\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3727\n",
      "Train loss: 0.24176782369613647\n",
      "Test loss: 0.2474975734949112\n",
      "----------------------\n",
      "Epoch: 3728\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3729\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3730\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3731\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3732\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3733\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3734\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3735\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3736\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3737\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3738\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3739\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3740\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3741\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3742\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3743\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3744\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3745\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3746\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3747\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3748\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3749\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3750\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3751\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3752\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975436925888\n",
      "----------------------\n",
      "Epoch: 3753\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3754\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3755\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3756\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3757\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3758\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3759\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3760\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3761\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3762\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3763\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3764\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3765\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3766\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3767\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3768\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3769\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3770\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3771\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3772\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3773\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3774\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3775\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3776\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3777\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3778\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3779\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3780\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3781\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3782\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3783\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3784\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3785\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3786\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3787\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3788\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3789\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3790\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3791\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3792\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3793\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3794\n",
      "Train loss: 0.2417677789926529\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3795\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3796\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3797\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3798\n",
      "Train loss: 0.2417677789926529\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3799\n",
      "Train loss: 0.2417677789926529\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3800\n",
      "Train loss: 0.2417677789926529\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3801\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3802\n",
      "Train loss: 0.2417677789926529\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3803\n",
      "Train loss: 0.2417677789926529\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3804\n",
      "Train loss: 0.2417677789926529\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3805\n",
      "Train loss: 0.2417677789926529\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3806\n",
      "Train loss: 0.2417677789926529\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3807\n",
      "Train loss: 0.2417677789926529\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3808\n",
      "Train loss: 0.2417677789926529\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3809\n",
      "Train loss: 0.2417677789926529\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3810\n",
      "Train loss: 0.24176780879497528\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3811\n",
      "Train loss: 0.2417677789926529\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3812\n",
      "Train loss: 0.2417677789926529\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3813\n",
      "Train loss: 0.2417677789926529\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3814\n",
      "Train loss: 0.2417677789926529\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3815\n",
      "Train loss: 0.2417677789926529\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3816\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3817\n",
      "Train loss: 0.2417677789926529\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3818\n",
      "Train loss: 0.2417677789926529\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3819\n",
      "Train loss: 0.2417677789926529\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3820\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3821\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3822\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3823\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3824\n",
      "Train loss: 0.2417677789926529\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3825\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3826\n",
      "Train loss: 0.2417677789926529\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3827\n",
      "Train loss: 0.2417677789926529\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3828\n",
      "Train loss: 0.2417677789926529\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3829\n",
      "Train loss: 0.2417677789926529\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3830\n",
      "Train loss: 0.2417677789926529\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3831\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3832\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3833\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3834\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3835\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3836\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3837\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3838\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3839\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3840\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3841\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3842\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3843\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3844\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3845\n",
      "Train loss: 0.2417677789926529\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3846\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3847\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3848\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3849\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3850\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3851\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.2474975287914276\n",
      "----------------------\n",
      "Epoch: 3852\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3853\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3854\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3855\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3856\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3857\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3858\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3859\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3860\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3861\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3862\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3863\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3864\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3865\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3866\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3867\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3868\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3869\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3870\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3871\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3872\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3873\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3874\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3875\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3876\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3877\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3878\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3879\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3880\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3881\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3882\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3883\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3884\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3885\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749751389026642\n",
      "----------------------\n",
      "Epoch: 3886\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3887\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3888\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3889\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3890\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3891\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3892\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3893\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3894\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3895\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3896\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3897\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3898\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3899\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3900\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3901\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3902\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3903\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3904\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3905\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3906\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3907\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3908\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3909\n",
      "Train loss: 0.2417677342891693\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3910\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3911\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3912\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3913\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3914\n",
      "Train loss: 0.2417677342891693\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3915\n",
      "Train loss: 0.2417677342891693\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3916\n",
      "Train loss: 0.2417677342891693\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3917\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3918\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3919\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3920\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3921\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3922\n",
      "Train loss: 0.2417677342891693\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3923\n",
      "Train loss: 0.2417677342891693\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3924\n",
      "Train loss: 0.2417677342891693\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3925\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3926\n",
      "Train loss: 0.2417677342891693\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3927\n",
      "Train loss: 0.2417677342891693\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3928\n",
      "Train loss: 0.2417677342891693\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3929\n",
      "Train loss: 0.2417677342891693\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3930\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3931\n",
      "Train loss: 0.2417677342891693\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3932\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3933\n",
      "Train loss: 0.2417677342891693\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3934\n",
      "Train loss: 0.2417677342891693\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3935\n",
      "Train loss: 0.2417677342891693\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3936\n",
      "Train loss: 0.2417677342891693\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3937\n",
      "Train loss: 0.2417677342891693\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3938\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3939\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3940\n",
      "Train loss: 0.2417677342891693\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3941\n",
      "Train loss: 0.2417677342891693\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3942\n",
      "Train loss: 0.2417677342891693\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3943\n",
      "Train loss: 0.2417677342891693\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3944\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3945\n",
      "Train loss: 0.2417677640914917\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3946\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3947\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3948\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3949\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3950\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3951\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3952\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3953\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3954\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3955\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3956\n",
      "Train loss: 0.2417677342891693\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3957\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3958\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3959\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3960\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3961\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3962\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3963\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3964\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3965\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3966\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3967\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3968\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3969\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3970\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3971\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3972\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3973\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3974\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3975\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3976\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3977\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3978\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3979\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3980\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3981\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3982\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3983\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3984\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3985\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3986\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3987\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749748408794403\n",
      "----------------------\n",
      "Epoch: 3988\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3989\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3990\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3991\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3992\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3993\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3994\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3995\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3996\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3997\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3998\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 3999\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4000\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4001\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4002\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4003\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4004\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4005\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4006\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4007\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4008\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4009\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4010\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4011\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4012\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4013\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4014\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4015\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4016\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4017\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4018\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4019\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4020\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4021\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4022\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4023\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4024\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4025\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4026\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4027\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4028\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4029\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4030\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4031\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4032\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4033\n",
      "Train loss: 0.24176768958568573\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4034\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4035\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4036\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4037\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4038\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4039\n",
      "Train loss: 0.24176768958568573\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4040\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4041\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4042\n",
      "Train loss: 0.24176768958568573\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4043\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4044\n",
      "Train loss: 0.24176768958568573\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4045\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4046\n",
      "Train loss: 0.24176768958568573\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4047\n",
      "Train loss: 0.24176768958568573\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4048\n",
      "Train loss: 0.24176768958568573\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4049\n",
      "Train loss: 0.24176768958568573\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4050\n",
      "Train loss: 0.24176768958568573\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4051\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4052\n",
      "Train loss: 0.24176768958568573\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4053\n",
      "Train loss: 0.24176768958568573\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4054\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749746918678284\n",
      "----------------------\n",
      "Epoch: 4055\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4056\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4057\n",
      "Train loss: 0.24176768958568573\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4058\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4059\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4060\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4061\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4062\n",
      "Train loss: 0.24176768958568573\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4063\n",
      "Train loss: 0.24176771938800812\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4064\n",
      "Train loss: 0.24176768958568573\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4065\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4066\n",
      "Train loss: 0.24176768958568573\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4067\n",
      "Train loss: 0.24176768958568573\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4068\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4069\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4070\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4071\n",
      "Train loss: 0.24176768958568573\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4072\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4073\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4074\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4075\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4076\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4077\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4078\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4079\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4080\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4081\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4082\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4083\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4084\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4085\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4086\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4087\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4088\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4089\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4090\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4091\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4092\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4093\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4094\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4095\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4096\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4097\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4098\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4099\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4100\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4101\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4102\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4103\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4104\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4105\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4106\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4107\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4108\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4109\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4110\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4111\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4112\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4113\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4114\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4115\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4116\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749743938446045\n",
      "----------------------\n",
      "Epoch: 4117\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4118\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4119\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4120\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4121\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4122\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4123\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4124\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4125\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4126\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4127\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4128\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4129\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4130\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4131\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4132\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4133\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4134\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4135\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4136\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4137\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4138\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4139\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4140\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4141\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4142\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4143\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4144\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4145\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4146\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4147\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4148\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4149\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4150\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4151\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4152\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4153\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4154\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4155\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4156\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4157\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4158\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4159\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4160\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4161\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4162\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4163\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4164\n",
      "Train loss: 0.24176765978336334\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4165\n",
      "Train loss: 0.24176765978336334\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4166\n",
      "Train loss: 0.24176765978336334\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4167\n",
      "Train loss: 0.24176765978336334\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4168\n",
      "Train loss: 0.24176765978336334\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4169\n",
      "Train loss: 0.24176765978336334\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4170\n",
      "Train loss: 0.24176765978336334\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4171\n",
      "Train loss: 0.24176765978336334\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4172\n",
      "Train loss: 0.24176765978336334\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4173\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4174\n",
      "Train loss: 0.24176765978336334\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4175\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4176\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4177\n",
      "Train loss: 0.24176765978336334\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4178\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4179\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4180\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4181\n",
      "Train loss: 0.24176767468452454\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4182\n",
      "Train loss: 0.24176765978336334\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4183\n",
      "Train loss: 0.24176765978336334\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4184\n",
      "Train loss: 0.24176765978336334\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4185\n",
      "Train loss: 0.24176765978336334\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4186\n",
      "Train loss: 0.24176765978336334\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4187\n",
      "Train loss: 0.24176765978336334\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4188\n",
      "Train loss: 0.24176765978336334\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4189\n",
      "Train loss: 0.24176765978336334\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4190\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4191\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4192\n",
      "Train loss: 0.24176765978336334\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4193\n",
      "Train loss: 0.24176765978336334\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4194\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4195\n",
      "Train loss: 0.24176765978336334\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4196\n",
      "Train loss: 0.24176765978336334\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4197\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749742448329926\n",
      "----------------------\n",
      "Epoch: 4198\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4199\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4200\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4201\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4202\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4203\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4204\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4205\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4206\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4207\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4208\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4209\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4210\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4211\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4212\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4213\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4214\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4215\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4216\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4217\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4218\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4219\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4220\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4221\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4222\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4223\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4224\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4225\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4226\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4227\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4228\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4229\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4230\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4231\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4232\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4233\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4234\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4235\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4236\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4237\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4238\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4239\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4240\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4241\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4242\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4243\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4244\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4245\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4246\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4247\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4248\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4249\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4250\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4251\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4252\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4253\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4254\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4255\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4256\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4257\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4258\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4259\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4260\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4261\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4262\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4263\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4264\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749739468097687\n",
      "----------------------\n",
      "Epoch: 4265\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4266\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4267\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4268\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4269\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4270\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4271\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4272\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4273\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4274\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4275\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4276\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4277\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4278\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4279\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4280\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4281\n",
      "Train loss: 0.24176761507987976\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4282\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4283\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4284\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4285\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4286\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4287\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4288\n",
      "Train loss: 0.24176761507987976\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4289\n",
      "Train loss: 0.24176761507987976\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4290\n",
      "Train loss: 0.24176761507987976\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4291\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4292\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4293\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4294\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4295\n",
      "Train loss: 0.24176761507987976\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4296\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4297\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4298\n",
      "Train loss: 0.24176761507987976\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4299\n",
      "Train loss: 0.24176761507987976\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4300\n",
      "Train loss: 0.24176761507987976\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4301\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4302\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4303\n",
      "Train loss: 0.24176761507987976\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4304\n",
      "Train loss: 0.24176761507987976\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4305\n",
      "Train loss: 0.24176761507987976\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4306\n",
      "Train loss: 0.24176761507987976\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4307\n",
      "Train loss: 0.24176761507987976\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4308\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4309\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4310\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4311\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749737977981567\n",
      "----------------------\n",
      "Epoch: 4312\n",
      "Train loss: 0.24176762998104095\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4313\n",
      "Train loss: 0.24176761507987976\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4314\n",
      "Train loss: 0.24176761507987976\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4315\n",
      "Train loss: 0.24176761507987976\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4316\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4317\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4318\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4319\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4320\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4321\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4322\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4323\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4324\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4325\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4326\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4327\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4328\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4329\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4330\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4331\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4332\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4333\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4334\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4335\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4336\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4337\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4338\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4339\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4340\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4341\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4342\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4343\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4344\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4345\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4346\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4347\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4348\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4349\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4350\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4351\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4352\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4353\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4354\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4355\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4356\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4357\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4358\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4359\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4360\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4361\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4362\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4363\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4364\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4365\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4366\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4367\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4368\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4369\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4370\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4371\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4372\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4373\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4374\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4375\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4376\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4377\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4378\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4379\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4380\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4381\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4382\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4383\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4384\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4385\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4386\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4387\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4388\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4389\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4390\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4391\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4392\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4393\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4394\n",
      "Train loss: 0.24176757037639618\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4395\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4396\n",
      "Train loss: 0.24176757037639618\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4397\n",
      "Train loss: 0.24176757037639618\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4398\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4399\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4400\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4401\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4402\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4403\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4404\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4405\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4406\n",
      "Train loss: 0.24176757037639618\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4407\n",
      "Train loss: 0.24176757037639618\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4408\n",
      "Train loss: 0.24176757037639618\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4409\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4410\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4411\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4412\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4413\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4414\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4415\n",
      "Train loss: 0.24176758527755737\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4416\n",
      "Train loss: 0.24176757037639618\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4417\n",
      "Train loss: 0.24176757037639618\n",
      "Test loss: 0.24749736487865448\n",
      "----------------------\n",
      "Epoch: 4418\n",
      "Train loss: 0.24176757037639618\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4419\n",
      "Train loss: 0.24176757037639618\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4420\n",
      "Train loss: 0.24176757037639618\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4421\n",
      "Train loss: 0.24176757037639618\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4422\n",
      "Train loss: 0.24176757037639618\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4423\n",
      "Train loss: 0.24176757037639618\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4424\n",
      "Train loss: 0.24176757037639618\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4425\n",
      "Train loss: 0.24176757037639618\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4426\n",
      "Train loss: 0.24176757037639618\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4427\n",
      "Train loss: 0.24176757037639618\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4428\n",
      "Train loss: 0.24176757037639618\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4429\n",
      "Train loss: 0.24176757037639618\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4430\n",
      "Train loss: 0.24176757037639618\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4431\n",
      "Train loss: 0.24176757037639618\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4432\n",
      "Train loss: 0.24176757037639618\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4433\n",
      "Train loss: 0.24176757037639618\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4434\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4435\n",
      "Train loss: 0.24176757037639618\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4436\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4437\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4438\n",
      "Train loss: 0.24176757037639618\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4439\n",
      "Train loss: 0.24176757037639618\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4440\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4441\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4442\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4443\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4444\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4445\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973350763321\n",
      "----------------------\n",
      "Epoch: 4446\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4447\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4448\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4449\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4450\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4451\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4452\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4453\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4454\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4455\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4456\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4457\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4458\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4459\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4460\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4461\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4462\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4463\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4464\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4465\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4466\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4467\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4468\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4469\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4470\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4471\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4472\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4473\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4474\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4475\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4476\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4477\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4478\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4479\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4480\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4481\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4482\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4483\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4484\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4485\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4486\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4487\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4488\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4489\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4490\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4491\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4492\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4493\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4494\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4495\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4496\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4497\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4498\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4499\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4500\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4501\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4502\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4503\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4504\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4505\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4506\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4507\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4508\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4509\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4510\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4511\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4512\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4513\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4514\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4515\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4516\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4517\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4518\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4519\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4520\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4521\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4522\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4523\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4524\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4525\n",
      "Train loss: 0.2417675405740738\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4526\n",
      "Train loss: 0.2417675256729126\n",
      "Test loss: 0.2474972903728485\n",
      "----------------------\n",
      "Epoch: 4527\n",
      "Train loss: 0.2417675256729126\n",
      "Test loss: 0.2474972903728485\n",
      "----------------------\n",
      "Epoch: 4528\n",
      "Train loss: 0.2417675256729126\n",
      "Test loss: 0.2474972903728485\n",
      "----------------------\n",
      "Epoch: 4529\n",
      "Train loss: 0.2417675256729126\n",
      "Test loss: 0.2474972903728485\n",
      "----------------------\n",
      "Epoch: 4530\n",
      "Train loss: 0.2417675256729126\n",
      "Test loss: 0.2474972903728485\n",
      "----------------------\n",
      "Epoch: 4531\n",
      "Train loss: 0.2417675256729126\n",
      "Test loss: 0.2474972903728485\n",
      "----------------------\n",
      "Epoch: 4532\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.2474972903728485\n",
      "----------------------\n",
      "Epoch: 4533\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.2474972903728485\n",
      "----------------------\n",
      "Epoch: 4534\n",
      "Train loss: 0.2417675256729126\n",
      "Test loss: 0.2474972903728485\n",
      "----------------------\n",
      "Epoch: 4535\n",
      "Train loss: 0.2417675256729126\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4536\n",
      "Train loss: 0.2417675256729126\n",
      "Test loss: 0.2474972903728485\n",
      "----------------------\n",
      "Epoch: 4537\n",
      "Train loss: 0.2417675256729126\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4538\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.2474972903728485\n",
      "----------------------\n",
      "Epoch: 4539\n",
      "Train loss: 0.2417675256729126\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4540\n",
      "Train loss: 0.2417675256729126\n",
      "Test loss: 0.2474972903728485\n",
      "----------------------\n",
      "Epoch: 4541\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4542\n",
      "Train loss: 0.2417675256729126\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4543\n",
      "Train loss: 0.2417675256729126\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4544\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4545\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4546\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.2474973201751709\n",
      "----------------------\n",
      "Epoch: 4547\n",
      "Train loss: 0.2417675256729126\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4548\n",
      "Train loss: 0.2417675256729126\n",
      "Test loss: 0.2474972903728485\n",
      "----------------------\n",
      "Epoch: 4549\n",
      "Train loss: 0.2417675256729126\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4550\n",
      "Train loss: 0.2417675256729126\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4551\n",
      "Train loss: 0.2417675256729126\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4552\n",
      "Train loss: 0.2417675256729126\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4553\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4554\n",
      "Train loss: 0.2417675256729126\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4555\n",
      "Train loss: 0.2417675256729126\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4556\n",
      "Train loss: 0.2417675256729126\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4557\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4558\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4559\n",
      "Train loss: 0.2417675256729126\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4560\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4561\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4562\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4563\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4564\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4565\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4566\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4567\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4568\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4569\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4570\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4571\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4572\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4573\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4574\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4575\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4576\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4577\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4578\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4579\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4580\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4581\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4582\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4583\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4584\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4585\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4586\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4587\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4588\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4589\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4590\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4591\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4592\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4593\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4594\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4595\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4596\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4597\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4598\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4599\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4600\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4601\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4602\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4603\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4604\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4605\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4606\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4607\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4608\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4609\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4610\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4611\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4612\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4613\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4614\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4615\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4616\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4617\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4618\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4619\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4620\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4621\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4622\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4623\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4624\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4625\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4626\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4627\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4628\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4629\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4630\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4631\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4632\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4633\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4634\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4635\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4636\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4637\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4638\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4639\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4640\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4641\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4642\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4643\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4644\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4645\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4646\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4647\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4648\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4649\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4650\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4651\n",
      "Train loss: 0.2417675107717514\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4652\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4653\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4654\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4655\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4656\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4657\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4658\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4659\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4660\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4661\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4662\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4663\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749727547168732\n",
      "----------------------\n",
      "Epoch: 4664\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4665\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4666\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4667\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4668\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4669\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4670\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4671\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4672\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4673\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4674\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4675\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4676\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4677\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4678\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4679\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4680\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4681\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4682\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4683\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4684\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4685\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4686\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4687\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4688\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4689\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4690\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4691\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4692\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4693\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4694\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4695\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4696\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4697\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4698\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4699\n",
      "Train loss: 0.24176748096942902\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4700\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4701\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4702\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4703\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4704\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4705\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4706\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4707\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749724566936493\n",
      "----------------------\n",
      "Epoch: 4708\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4709\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4710\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4711\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4712\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4713\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4714\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4715\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4716\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4717\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4718\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4719\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4720\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4721\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4722\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4723\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4724\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4725\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4726\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4727\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4728\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4729\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4730\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4731\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4732\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4733\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4734\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4735\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4736\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4737\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4738\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4739\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4740\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4741\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4742\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4743\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4744\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4745\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4746\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4747\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4748\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4749\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4750\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4751\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4752\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4753\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4754\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4755\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4756\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4757\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4758\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4759\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4760\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4761\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4762\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4763\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4764\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4765\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4766\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4767\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4768\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4769\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4770\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4771\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4772\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4773\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4774\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4775\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4776\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4777\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4778\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4779\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4780\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4781\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4782\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4783\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4784\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4785\n",
      "Train loss: 0.24176746606826782\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4786\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4787\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4788\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4789\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4790\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4791\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4792\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4793\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4794\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4795\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4796\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4797\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4798\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4799\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4800\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4801\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4802\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4803\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4804\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4805\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4806\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4807\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4808\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4809\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4810\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4811\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4812\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4813\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4814\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4815\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4816\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4817\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4818\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4819\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4820\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4821\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4822\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4823\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4824\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4825\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749723076820374\n",
      "----------------------\n",
      "Epoch: 4826\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4827\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4828\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4829\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4830\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4831\n",
      "Train loss: 0.24176743626594543\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4832\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4833\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4834\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4835\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4836\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4837\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4838\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4839\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4840\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4841\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4842\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4843\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4844\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4845\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4846\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4847\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4848\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4849\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4850\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4851\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4852\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4853\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4854\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4855\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4856\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4857\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4858\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4859\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4860\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4861\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4862\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4863\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4864\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4865\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4866\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4867\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4868\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749721586704254\n",
      "----------------------\n",
      "Epoch: 4869\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4870\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4871\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4872\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4873\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4874\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4875\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4876\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4877\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4878\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4879\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4880\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4881\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4882\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4883\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4884\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4885\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4886\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4887\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4888\n",
      "Train loss: 0.24176739156246185\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4889\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4890\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4891\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4892\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4893\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4894\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4895\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4896\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4897\n",
      "Train loss: 0.24176739156246185\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4898\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4899\n",
      "Train loss: 0.24176739156246185\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4900\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4901\n",
      "Train loss: 0.24176739156246185\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4902\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4903\n",
      "Train loss: 0.24176739156246185\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4904\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4905\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4906\n",
      "Train loss: 0.24176739156246185\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4907\n",
      "Train loss: 0.24176739156246185\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4908\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4909\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4910\n",
      "Train loss: 0.24176739156246185\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4911\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4912\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4913\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4914\n",
      "Train loss: 0.24176739156246185\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4915\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4916\n",
      "Train loss: 0.24176739156246185\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4917\n",
      "Train loss: 0.24176739156246185\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4918\n",
      "Train loss: 0.24176739156246185\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4919\n",
      "Train loss: 0.24176739156246185\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4920\n",
      "Train loss: 0.24176742136478424\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4921\n",
      "Train loss: 0.24176739156246185\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4922\n",
      "Train loss: 0.24176739156246185\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4923\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4924\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4925\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4926\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4927\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4928\n",
      "Train loss: 0.24176739156246185\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4929\n",
      "Train loss: 0.24176739156246185\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4930\n",
      "Train loss: 0.24176739156246185\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4931\n",
      "Train loss: 0.24176739156246185\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4932\n",
      "Train loss: 0.24176739156246185\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4933\n",
      "Train loss: 0.24176739156246185\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4934\n",
      "Train loss: 0.24176739156246185\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4935\n",
      "Train loss: 0.24176739156246185\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4936\n",
      "Train loss: 0.24176739156246185\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4937\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4938\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749718606472015\n",
      "----------------------\n",
      "Epoch: 4939\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4940\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4941\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4942\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4943\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4944\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4945\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4946\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4947\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4948\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4949\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4950\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4951\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4952\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4953\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4954\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4955\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4956\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4957\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4958\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4959\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4960\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4961\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4962\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4963\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4964\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4965\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4966\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4967\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4968\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4969\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4970\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4971\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4972\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749717116355896\n",
      "----------------------\n",
      "Epoch: 4973\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4974\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4975\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4976\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4977\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4978\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4979\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4980\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4981\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4982\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4983\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4984\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4985\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4986\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4987\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4988\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4989\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4990\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4991\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4992\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4993\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4994\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4995\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4996\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4997\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4998\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n",
      "Epoch: 4999\n",
      "Train loss: 0.24176737666130066\n",
      "Test loss: 0.24749714136123657\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "epochs = 5000\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_preds = -1\n",
    "test_preds = -1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    train_preds = train_step(model, loss_fn, optimizer, X_train, y_train)\n",
    "    test_preds = test_step(model, loss_fn, X_test, y_test)\n",
    "    print(\"----------------------\")\n",
    "\n",
    "train_preds = train_preds * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Train preds')"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "relu_fn = nn.ReLU()\n",
    "\n",
    "# train_preds = relu_fn(train_preds * 100)\n",
    "\n",
    "np_train_preds = [data.cpu().detach().numpy() for data in relu_fn(train_preds)]\n",
    "np_train_actual = [data.cpu().detach().numpy() for data in y_train]\n",
    "np_test_preds = [data.cpu().detach().numpy() for data in relu_fn(test_preds)]\n",
    "np_test_actual = [data.cpu().detach().numpy() for data in y_test]\n",
    "\n",
    "\n",
    "plt.plot(range(len(np_train_preds)), np_train_preds)\n",
    "# plt.plot(range(len(np_train_actual)), np_train_actual)\n",
    "plt.title(\"Train preds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Train actual')"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(np_train_actual)), np_train_actual)\n",
    "plt.title(\"Train actual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Test preds')"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(np_test_preds)), np_test_preds)\n",
    "plt.plot(range(len(np_test_actual)), np_test_actual)\n",
    "plt.title(\"Test preds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Test actual')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(np_test_actual)), np_test_actual)\n",
    "plt.title(\"Test actual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (100,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m np_train_losses \u001b[38;5;241m=\u001b[39m [data\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_losses]\n\u001b[0;32m----> 6\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m), np_train_losses)\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/matplotlib/pyplot.py:3578\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3570\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   3571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[1;32m   3572\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3577\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[0;32m-> 3578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gca()\u001b[38;5;241m.\u001b[39mplot(\n\u001b[1;32m   3579\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m   3580\u001b[0m         scalex\u001b[38;5;241m=\u001b[39mscalex,\n\u001b[1;32m   3581\u001b[0m         scaley\u001b[38;5;241m=\u001b[39mscaley,\n\u001b[1;32m   3582\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[1;32m   3583\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3584\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/matplotlib/axes/_axes.py:1721\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1480\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1720\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1721\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1723\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/matplotlib/axes/_base.py:303\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    302\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plot_args(\n\u001b[1;32m    304\u001b[0m     axes, this, kwargs, ambiguous_fmt_datakey\u001b[38;5;241m=\u001b[39mambiguous_fmt_datakey)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/matplotlib/axes/_base.py:499\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    496\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    500\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    503\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (100,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAGiCAYAAADA0E3hAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcu0lEQVR4nO3db2yV5f348U9paaturRG0FkEEpxMl6mgDo6wandag0ZBskcVF1GliszmETqeMRYYxaXTRfXUKbgoaE3REReeDztEHG1Zxf2DFGCFxEWZBW0kxtqhbGXD/Hhj6W9fiOLV/uNrXK7kfnMv7Puc6uazn7X2fP3lZlmUBAJCAMcM9AQCAIyVcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGTkHC6vvPJKXHnllTFhwoTIy8uLF1988X8es2HDhqioqIji4uKYOnVqPProo/2ZKwAwyuUcLp988kmcd9558fDDDx/R/jt27IjLL788qquro7m5OX7yk5/EwoUL4/nnn895sgDA6Jb3RX5kMS8vL1544YWYN2/eYfe544474qWXXopt27Z1j9XW1sYbb7wRr7/+en8fGgAYhQoG+wFef/31qKmp6TF22WWXxapVq+Lf//53jB07ttcxXV1d0dXV1X374MGD8eGHH8a4ceMiLy9vsKcMAAyALMti7969MWHChBgzZmDeVjvo4dLW1hZlZWU9xsrKymL//v3R3t4e5eXlvY6pr6+P5cuXD/bUAIAhsHPnzpg4ceKA3Negh0tE9DpLcujq1OHOnixZsiTq6uq6b3d0dMSpp54aO3fujJKSksGbKAAwYDo7O2PSpEnx5S9/ecDuc9DD5eSTT462trYeY7t3746CgoIYN25cn8cUFRVFUVFRr/GSkhLhAgCJGci3eQz697jMnj07Ghsbe4ytX78+Kisr+3x/CwDA4eQcLh9//HFs2bIltmzZEhGffdx5y5Yt0dLSEhGfXeZZsGBB9/61tbXx7rvvRl1dXWzbti1Wr14dq1atittuu21gngEAMGrkfKlo06ZNcdFFF3XfPvRelOuuuy6efPLJaG1t7Y6YiIgpU6ZEQ0NDLF68OB555JGYMGFCPPTQQ/Gtb31rAKYPAIwmX+h7XIZKZ2dnlJaWRkdHh/e4AEAiBuP1228VAQDJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQjH6Fy4oVK2LKlClRXFwcFRUV0dTU9Ln7r1mzJs4777w49thjo7y8PG644YbYs2dPvyYMAIxeOYfL2rVrY9GiRbF06dJobm6O6urqmDt3brS0tPS5/6uvvhoLFiyIG2+8Md5666149tln469//WvcdNNNX3jyAMDoknO4PPDAA3HjjTfGTTfdFNOmTYv/+7//i0mTJsXKlSv73P9Pf/pTnHbaabFw4cKYMmVKfOMb34ibb745Nm3a9IUnDwCMLjmFy759+2Lz5s1RU1PTY7ympiY2btzY5zFVVVWxa9euaGhoiCzL4oMPPojnnnsurrjiisM+TldXV3R2dvbYAAByCpf29vY4cOBAlJWV9RgvKyuLtra2Po+pqqqKNWvWxPz586OwsDBOPvnkOP744+OXv/zlYR+nvr4+SktLu7dJkyblMk0AYITq15tz8/LyetzOsqzX2CFbt26NhQsXxl133RWbN2+Ol19+OXbs2BG1tbWHvf8lS5ZER0dH97Zz587+TBMAGGEKctl5/PjxkZ+f3+vsyu7du3udhTmkvr4+5syZE7fffntERJx77rlx3HHHRXV1ddxzzz1RXl7e65iioqIoKirKZWoAwCiQ0xmXwsLCqKioiMbGxh7jjY2NUVVV1ecxn376aYwZ0/Nh8vPzI+KzMzUAAEcq50tFdXV18fjjj8fq1atj27ZtsXjx4mhpaem+9LNkyZJYsGBB9/5XXnllrFu3LlauXBnbt2+P1157LRYuXBgzZ86MCRMmDNwzAQBGvJwuFUVEzJ8/P/bs2RN33313tLa2xvTp06OhoSEmT54cERGtra09vtPl+uuvj71798bDDz8cP/rRj+L444+Piy++OO69996BexYAwKiQlyVwvaazszNKS0ujo6MjSkpKhns6AMARGIzXb79VBAAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMvoVLitWrIgpU6ZEcXFxVFRURFNT0+fu39XVFUuXLo3JkydHUVFRnH766bF69ep+TRgAGL0Kcj1g7dq1sWjRolixYkXMmTMnfvWrX8XcuXNj69atceqpp/Z5zNVXXx0ffPBBrFq1Kr7yla/E7t27Y//+/V948gDA6JKXZVmWywGzZs2KGTNmxMqVK7vHpk2bFvPmzYv6+vpe+7/88svxne98J7Zv3x4nnHBCvybZ2dkZpaWl0dHRESUlJf26DwBgaA3G63dOl4r27dsXmzdvjpqamh7jNTU1sXHjxj6Peemll6KysjLuu+++OOWUU+LMM8+M2267Lf75z38e9nG6urqis7OzxwYAkNOlovb29jhw4ECUlZX1GC8rK4u2trY+j9m+fXu8+uqrUVxcHC+88EK0t7fH97///fjwww8P+z6X+vr6WL58eS5TAwBGgX69OTcvL6/H7SzLeo0dcvDgwcjLy4s1a9bEzJkz4/LLL48HHnggnnzyycOedVmyZEl0dHR0bzt37uzPNAGAESanMy7jx4+P/Pz8XmdXdu/e3esszCHl5eVxyimnRGlpaffYtGnTIsuy2LVrV5xxxhm9jikqKoqioqJcpgYAjAI5nXEpLCyMioqKaGxs7DHe2NgYVVVVfR4zZ86ceP/99+Pjjz/uHnv77bdjzJgxMXHixH5MGQAYrXK+VFRXVxePP/54rF69OrZt2xaLFy+OlpaWqK2tjYjPLvMsWLCge/9rrrkmxo0bFzfccENs3bo1Xnnllbj99tvje9/7XhxzzDED90wAgBEv5+9xmT9/fuzZsyfuvvvuaG1tjenTp0dDQ0NMnjw5IiJaW1ujpaWle/8vfelL0djYGD/84Q+jsrIyxo0bF1dffXXcc889A/csAIBRIefvcRkOvscFANIz7N/jAgAwnIQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJKNf4bJixYqYMmVKFBcXR0VFRTQ1NR3Rca+99loUFBTE+eef35+HBQBGuZzDZe3atbFo0aJYunRpNDc3R3V1dcydOzdaWlo+97iOjo5YsGBBfPOb3+z3ZAGA0S0vy7IslwNmzZoVM2bMiJUrV3aPTZs2LebNmxf19fWHPe473/lOnHHGGZGfnx8vvvhibNmy5bD7dnV1RVdXV/ftzs7OmDRpUnR0dERJSUku0wUAhklnZ2eUlpYO6Ot3Tmdc9u3bF5s3b46ampoe4zU1NbFx48bDHvfEE0/EO++8E8uWLTuix6mvr4/S0tLubdKkSblMEwAYoXIKl/b29jhw4ECUlZX1GC8rK4u2trY+j/n73/8ed955Z6xZsyYKCgqO6HGWLFkSHR0d3dvOnTtzmSYAMEIdWUn8l7y8vB63syzrNRYRceDAgbjmmmti+fLlceaZZx7x/RcVFUVRUVF/pgYAjGA5hcv48eMjPz+/19mV3bt39zoLExGxd+/e2LRpUzQ3N8ctt9wSEREHDx6MLMuioKAg1q9fHxdffPEXmD4AMJrkdKmosLAwKioqorGxscd4Y2NjVFVV9dq/pKQk3nzzzdiyZUv3VltbG1/96ldjy5YtMWvWrC82ewBgVMn5UlFdXV1ce+21UVlZGbNnz45f//rX0dLSErW1tRHx2ftT3nvvvXjqqadizJgxMX369B7Hn3TSSVFcXNxrHADgf8k5XObPnx979uyJu+++O1pbW2P69OnR0NAQkydPjoiI1tbW//mdLgAA/ZHz97gMh8H4HDgAMLiG/XtcAACGk3ABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZPQrXFasWBFTpkyJ4uLiqKioiKampsPuu27durj00kvjxBNPjJKSkpg9e3b8/ve/7/eEAYDRK+dwWbt2bSxatCiWLl0azc3NUV1dHXPnzo2WlpY+93/llVfi0ksvjYaGhti8eXNcdNFFceWVV0Zzc/MXnjwAMLrkZVmW5XLArFmzYsaMGbFy5crusWnTpsW8efOivr7+iO7jnHPOifnz58ddd93V5z/v6uqKrq6u7tudnZ0xadKk6OjoiJKSklymCwAMk87OzigtLR3Q1++czrjs27cvNm/eHDU1NT3Ga2pqYuPGjUd0HwcPHoy9e/fGCSeccNh96uvro7S0tHubNGlSLtMEAEaonMKlvb09Dhw4EGVlZT3Gy8rKoq2t7Yju4/77749PPvkkrr766sPus2TJkujo6Ojedu7cmcs0AYARqqA/B+Xl5fW4nWVZr7G+PPPMM/Gzn/0sfvvb38ZJJ5102P2KioqiqKioP1MDAEawnMJl/PjxkZ+f3+vsyu7du3udhflva9eujRtvvDGeffbZuOSSS3KfKQAw6uV0qaiwsDAqKiqisbGxx3hjY2NUVVUd9rhnnnkmrr/++nj66afjiiuu6N9MAYBRL+dLRXV1dXHttddGZWVlzJ49O379619HS0tL1NbWRsRn709577334qmnnoqIz6JlwYIF8eCDD8bXv/717rM1xxxzTJSWlg7gUwEARrqcw2X+/PmxZ8+euPvuu6O1tTWmT58eDQ0NMXny5IiIaG1t7fGdLr/61a9i//798YMf/CB+8IMfdI9fd9118eSTT37xZwAAjBo5f4/LcBiMz4EDAINr2L/HBQBgOAkXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASEa/wmXFihUxZcqUKC4ujoqKimhqavrc/Tds2BAVFRVRXFwcU6dOjUcffbRfkwUARrecw2Xt2rWxaNGiWLp0aTQ3N0d1dXXMnTs3Wlpa+tx/x44dcfnll0d1dXU0NzfHT37yk1i4cGE8//zzX3jyAMDokpdlWZbLAbNmzYoZM2bEypUru8emTZsW8+bNi/r6+l7733HHHfHSSy/Ftm3busdqa2vjjTfeiNdff73Px+jq6oqurq7u2x0dHXHqqafGzp07o6SkJJfpAgDDpLOzMyZNmhQfffRRlJaWDsydZjno6urK8vPzs3Xr1vUYX7hwYXbBBRf0eUx1dXW2cOHCHmPr1q3LCgoKsn379vV5zLJly7KIsNlsNpvNNgK2d955J5fc+FwFkYP29vY4cOBAlJWV9RgvKyuLtra2Po9pa2vrc//9+/dHe3t7lJeX9zpmyZIlUVdX1337o48+ismTJ0dLS8vAFRv9cqienf0aftbi6GEtji7W4+hx6IrJCSecMGD3mVO4HJKXl9fjdpZlvcb+1/59jR9SVFQURUVFvcZLS0v9S3iUKCkpsRZHCWtx9LAWRxfrcfQYM2bgPsSc0z2NHz8+8vPze51d2b17d6+zKoecfPLJfe5fUFAQ48aNy3G6AMBollO4FBYWRkVFRTQ2NvYYb2xsjKqqqj6PmT17dq/9169fH5WVlTF27NgcpwsAjGY5n7upq6uLxx9/PFavXh3btm2LxYsXR0tLS9TW1kbEZ+9PWbBgQff+tbW18e6770ZdXV1s27YtVq9eHatWrYrbbrvtiB+zqKgoli1b1uflI4aWtTh6WIujh7U4uliPo8dgrEXOH4eO+OwL6O67775obW2N6dOnxy9+8Yu44IILIiLi+uuvj3/84x/xxz/+sXv/DRs2xOLFi+Ott96KCRMmxB133NEdOgAAR6pf4QIAMBz8VhEAkAzhAgAkQ7gAAMkQLgBAMo6acFmxYkVMmTIliouLo6KiIpqamj53/w0bNkRFRUUUFxfH1KlT49FHHx2imY58uazFunXr4tJLL40TTzwxSkpKYvbs2fH73/9+CGc7suX6d3HIa6+9FgUFBXH++ecP7gRHkVzXoqurK5YuXRqTJ0+OoqKiOP3002P16tVDNNuRLde1WLNmTZx33nlx7LHHRnl5edxwww2xZ8+eIZrtyPXKK6/ElVdeGRMmTIi8vLx48cUX/+cxA/LaPWC/evQF/OY3v8nGjh2bPfbYY9nWrVuzW2+9NTvuuOOyd999t8/9t2/fnh177LHZrbfemm3dujV77LHHsrFjx2bPPffcEM985Ml1LW699dbs3nvvzf7yl79kb7/9drZkyZJs7Nix2d/+9rchnvnIk+taHPLRRx9lU6dOzWpqarLzzjtvaCY7wvVnLa666qps1qxZWWNjY7Zjx47sz3/+c/baa68N4axHplzXoqmpKRszZkz24IMPZtu3b8+ampqyc845J5s3b94Qz3zkaWhoyJYuXZo9//zzWURkL7zwwufuP1Cv3UdFuMycOTOrra3tMXbWWWdld955Z5/7//jHP87OOuusHmM333xz9vWvf33Q5jha5LoWfTn77LOz5cuXD/TURp3+rsX8+fOzn/70p9myZcuEywDJdS1+97vfZaWlpdmePXuGYnqjSq5r8fOf/zybOnVqj7GHHnoomzhx4qDNcTQ6knAZqNfuYb9UtG/fvti8eXPU1NT0GK+pqYmNGzf2eczrr7/ea//LLrssNm3aFP/+978Hba4jXX/W4r8dPHgw9u7dO6C/BDoa9XctnnjiiXjnnXdi2bJlgz3FUaM/a/HSSy9FZWVl3HfffXHKKafEmWeeGbfddlv885//HIopj1j9WYuqqqrYtWtXNDQ0RJZl8cEHH8Rzzz0XV1xxxVBMmf8wUK/d/fp16IHU3t4eBw4c6PUjjWVlZb1+nPGQtra2Pvffv39/tLe3R3l5+aDNdyTrz1r8t/vvvz8++eSTuPrqqwdjiqNGf9bi73//e9x5553R1NQUBQXD/qc9YvRnLbZv3x6vvvpqFBcXxwsvvBDt7e3x/e9/Pz788EPvc/kC+rMWVVVVsWbNmpg/f37861//iv3798dVV10Vv/zlL4diyvyHgXrtHvYzLofk5eX1uJ1lWa+x/7V/X+PkLte1OOSZZ56Jn/3sZ7F27do46aSTBmt6o8qRrsWBAwfimmuuieXLl8eZZ545VNMbVXL5uzh48GDk5eXFmjVrYubMmXH55ZfHAw88EE8++aSzLgMgl7XYunVrLFy4MO66667YvHlzvPzyy7Fjxw4/OzNMBuK1e9j/t2z8+PGRn5/fq5Z3797dq8wOOfnkk/vcv6CgIMaNGzdocx3p+rMWh6xduzZuvPHGePbZZ+OSSy4ZzGmOCrmuxd69e2PTpk3R3Nwct9xyS0R89uKZZVkUFBTE+vXr4+KLLx6SuY80/fm7KC8vj1NOOSVKS0u7x6ZNmxZZlsWuXbvijDPOGNQ5j1T9WYv6+vqYM2dO3H777RERce6558Zxxx0X1dXVcc899zhDP4QG6rV72M+4FBYWRkVFRTQ2NvYYb2xsjKqqqj6PmT17dq/9169fH5WVlTF27NhBm+tI15+1iPjsTMv1118fTz/9tOvGAyTXtSgpKYk333wztmzZ0r3V1tbGV7/61diyZUvMmjVrqKY+4vTn72LOnDnx/vvvx8cff9w99vbbb8eYMWNi4sSJgzrfkaw/a/Hpp5/GmDE9X+ry8/Mj4v//3z5DY8Beu3N6K+8gOfTxtlWrVmVbt27NFi1alB133HHZP/7xjyzLsuzOO+/Mrr322u79D32kavHixdnWrVuzVatW+Tj0AMl1LZ5++umsoKAge+SRR7LW1tbu7aOPPhqupzBi5LoW/82nigZOrmuxd+/ebOLEidm3v/3t7K233so2bNiQnXHGGdlNN900XE9hxMh1LZ544omsoKAgW7FiRfbOO+9kr776alZZWZnNnDlzuJ7CiLF3796subk5a25uziIie+CBB7Lm5ubuj6YP1mv3UREuWZZljzzySDZ58uSssLAwmzFjRrZhw4buf3bddddlF154YY/9//jHP2Zf+9rXssLCwuy0007LVq5cOcQzHrlyWYsLL7wwi4he23XXXTf0Ex+Bcv27+E/CZWDluhbbtm3LLrnkkuyYY47JJk6cmNXV1WWffvrpEM96ZMp1LR566KHs7LPPzo455pisvLw8++53v5vt2rVriGc98vzhD3/43P/+D9Zrd16WOVcGAKRh2N/jAgBwpIQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAk4/8BrQWhjBP+6s8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np_train_losses = [data.cpu().detach().numpy() for data in train_losses]\n",
    "\n",
    "\n",
    "plt.plot(range(100), np_train_losses)\n",
    "plt.title(\"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: models/dengue_model.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 1. Create models directory\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. Create model save path\n",
    "MODEL_NAME = \"dengue_model2.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "# 3. Sve the model state dict\n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=model.state_dict(), f=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Test loss')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np_test_losses = [data.cpu().detach().numpy() for data in test_losses]\n",
    "plt.plot(range(100), np_test_losses)\n",
    "plt.title(\"Test loss\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
